---
title: 数据密集型应用系统设计（7-8章）
date: 2025-09-28 20:39:32
categories:
   - [Read-Note, 数据密集型应用系统设计]
---



# 分布式系统


## 九、一致性与共识

![](/images/diagram-4.png)

类比事务的抽象可以让应用程序可以假装没有崩溃（原⼦性），没有与其他⼈并发访问数据库（隔离性），且存储设备是完全可靠的（持久性），尝试建立可以让分布式系统应用忽略内部各种问题的抽象机制

**重要抽象之一：共识**

分布式系统可提供的若干保证和抽象机制  -->  解决共识问题的相关算法



### 一致性保证

最终一致性：停止更新数据库，等待一段时间后最终所有读请求会返回相同的内容。即不一致现象是暂时的，最终会达到一致

更强的一致性模型，意味着更多代价，如性能降低或容错性降低



### 可线性化

![非线性化例子](</images/截屏2025-06-27 14.35.10.png>)

可线性化基本思想：**使系统看起来好像只有一个数据副本**

![一个重要约束](</images/截屏2025-06-27 14.56.19.png>)

![](</images/截屏2025-06-27 15.01.41.png>)

CAP 理论，在出现网络故障时，一致性（线性化）和可用性无法兼具

线性化和性能之间的取舍



### 顺序保证

线性化的操作按某种顺序执行（图 9-4）

一些顺序要求的场景：

* 主从复制时，主节点确定复制日志写入顺序

* 可串行化确保的事务执行结果与按照某种顺序方式执行一样。

#### 顺序与因果关系

顺序的作用：保障因果关系

要求因果关系的一些场景：

* 图 5-5，先有问题的答案，再有问题的本身，违背了因果关系

* 图 5-9，需要先有创建数据行，后有更新

* 第 5 章“检测并发写”，两个操作 A 和 B，三种关系，A 发生在 B 之前，B 发生在 A 之前，A 和 B 并发，前两者都是因果关系

* 事务快照隔离，事务从一致性快照读取，一致性即与因果关系一直，如果快照包含了答案，那么它也应该包含所提的问题

* 事务写倾斜，图 7-8 调班动作的因果关系取决于当前值班人，可序列化的快照隔离（可串行化的快照隔离）主要通过跟踪事务之间的因果依赖关系来达到检测写倾斜目的

**因果顺序并⾮全序**

全序：支持任意两个元素的比较

因果：某两个事件的关系，一个发生在另一个前

#### 序列号排序

使用序列号排序事件

存在唯一主节点，可以让主节点为每个操作递增某个计数器，从而为复制日志中的每个操作赋值一个单增的序列号

**非因果序列发生器**

不存在唯一主节点，产生序列号较复杂

三个思路

1. 每个节点独立产生自己的一组序列号，比如 A 产生奇数，B 产生偶数，或者 A 产生 1-1000，B 产生 1000-2000

2. 时间戳信息（物理时钟）附加到操作上

问题：产生的序列号与因果关系不严格一致

**Lamport 时间戳**

值对（计数器，节点 ID）

![](/images/image-36.png)

最大计数器值嵌入到每一个请求中，如果发现收到的响应或请求中的最大值大于自身的技术器值，则把自己的计数器修改为该最大值

#### 全序关系广播

比喻：想象一个聊天群，里面有多个成员。全序关系广播要保证：无论消息从哪个成员发出，也无论每个成员在什么时间收到消息，所有成员看到的聊天记录顺序必须完全一致。

全序关系广播是一种消息传递协议，它确保在分布式系统的所有节点上，消息的传递顺序是完全一致的。它需要满足以下**两个核心属性**：

1. 可靠广播：如果一个正确的（非故障的）节点广播了一条消息，那么所有正确的节点最终都会收到这条消息。（不会丢失消息）

2. 全序传递：任何两个正确的节点，它们收到所有消息的顺序都是完全相同的。

**为什么需要它？**

在分布式系统（如数据库、状态机）中，多个副本需要保持完全一致的状态。实现这一目标最有效的方法就是让所有副本都以相同的顺序执行相同的操作指令。全序广播正是用来保证所有副本收到操作指令的顺序一致的机制。

实现技术：

常见的共识算法，如 Paxos 和 Raft，其核心功能之一就是实现全序关系广播。在 Raft 中，领导者（Leader）负责为所有客户端请求（即日志条目）确定一个唯一的顺序，并将这个顺序复制到所有追随者（Follower）节点上。



全序关系广播是实现线性化存储的一种强大工具。

* 如果你有一个全序关系广播系统，你可以很容易地构建一个线性化的存储。

  * 方法：将所有写操作（有时也包括读操作）通过全序广播发送给所有副本。每个副本按照广播确定的全局唯一顺序来依次执行这些操作。由于所有副本以相同顺序执行相同操作，它们的最终状态会保持一致，并且整个系统对外表现为线性化。



### 分布式事务与共识

需要达成一致的场景

* 主节点选举

* 原子事务提交

#### 原子提交和2PC（两阶段提交）

![](/images/image-32.png)

![](/images/image-31.png)

#### 实践中的分布式事务

##### **Exactly-once 消息处理**

* 在分布式系统中，消息可能因为网络抖动或服务故障而被**重复发送**。

* 如果处理逻辑不是幂等的（idempotent），重复处理会导致数据错误。

* Exactly-once 的目标是：**无论消息被发送多少次，处理结果都只发生一次且正确。**

**⚙️ 技术策略：如何实现 Exactly-once？**

1. **使用唯一消息 ID（Message ID）**

   * 每条消息附带一个唯一标识符。

   * 消费者记录已处理的 ID，避免重复处理。

2. **幂等操作（Idempotent Operations）**

   * 设计处理逻辑，使得重复执行不会改变最终结果。

   * 例如：数据库插入使用 UPSERT、更新使用 SET 而非 INCREMENT。

3. **事务性处理（Transactional Processing）**

   * 将消息消费与状态更新放入同一个事务中。

   * 如果事务失败，消息不会被标记为已处理。

4. **去重存储（Deduplication Store）**

   * 使用 Redis、数据库或日志系统记录已处理消息 ID。

   * 注意存储的 TTL 和性能影响。

**📌 实践挑战与权衡**

* **性能 vs. 精度**：Exactly-once 通常比 At-least-once 慢，因为需要额外的存储和检查。

* **状态一致性**：消费者必须维护处理状态，可能引入复杂的状态同步问题。

* **系统设计**：Kafka、RabbitMQ 等消息系统本身不保证 Exactly-once，需要应用层配合。

##### **XA交易**

**XA（eXtended Architecture）** 是由 X/Open 标准制定的分布式事务处理协议。

它用于在多个数据库或消息系统之间协调事务，确保满足 **ACID**（原子性、一致性、隔离性、持久性）属性。

支持 XA 的系统和工具

* 常见数据库：PostgreSQL、MySQL、DB2、SQL Server、Oracle 等。

* 消息中间件：ActiveMQ、HornetQ、MSMQ、IBM MQ 等

XA 使用 **两阶段提交协议（2PC）** 来确保所有参与者一致提交或回滚事务。

事务管理器负责协调各个资源的准备和提交过程。



##### **协调者故障中恢复**

当系统组件发生故障时，恢复不仅仅是重启，而是要**恢复到一致的状态**。

为了实现这一目标，系统需要依赖**日志记录**和**协调机制**。

**📚 关键概念拆解**

1. **日志的重要性**：

   * 每个组件记录自己的操作日志。

   * 恢复时通过重放日志来恢复状态。

   * 但日志可能不完整或顺序不一致，需协调。

2. **一致性判断的挑战**：

   * 系统需要判断哪些操作已经完成，哪些未完成。

   * 如果没有协调机制，可能会出现状态不一致或数据丢失。

3. **协调机制的作用**：

   * 协调多个组件的日志与状态，确保恢复后系统整体一致。

   * 类似于分布式事务中的两阶段提交（2PC）或更复杂的共识协议（如 Paxos、Raft）。

**实践中的恢复策略**

* **检查点（Checkpoint）**：定期保存系统状态，减少恢复时的复杂度。

* **幂等性设计**：确保操作可以重复执行而不会造成副作用。

* **恢复流程的验证**：系统设计中必须考虑如何验证恢复的正确性。

##### 分布式事务限制

* **分布式事务（如 XA）虽然能提供强一致性，但代价高昂**。

* 在大规模系统中，XA 的性能瓶颈和复杂性使其难以落地。

* 许多现代系统（如 HDFS、HBase）选择放弃 XA，采用更灵活的方式处理一致性。

**XA 的问题**：

* 实现复杂，依赖底层资源管理器的支持。

* 性能开销大，尤其在高并发场景下。

* 容错能力弱，一旦协调者故障，可能导致事务阻塞。

**2PC 的局限性**：

* **阻塞问题**：参与者在等待协调者指令时无法继续处理其他事务。

* **单点故障**：协调者崩溃后，系统可能陷入不确定状态。

* **缺乏自动恢复机制**：无法应对网络分区或节点重启后的状态恢复。

**一致性 vs 可用性**：

* 在 CAP 理论下，强一致性往往牺牲了可用性。

* 某些系统选择最终一致性或幂等操作来提升可用性。



#### 支持容错的共识

* 在分布式系统中，多个节点需要就某个事实达成一致（consensus），即使存在网络延迟、节点故障或恶意行为。

* 共识协议的目标是确保系统在不可靠环境下仍能做出一致决策。

**📐 共识协议的基本性质**

1. **一致性（Uniform Agreement）**

   * 所有非故障节点最终必须达成相同的决策。

2. **有效性（Validity）**

   * 如果某个节点提出了一个值，最终决定的值必须是某个节点曾提出过的。

3. **终止性（Termination）**

   * 所有非故障节点最终都必须做出决定，不能无限等待。

##### 共识算法与全序广播

* **共识算法（Consensus Algorithm）**：用于在多个节点之间就某个值达成一致，确保系统状态一致性。

* **全序广播（Total Order Broadcast）**：确保所有节点以相同顺序接收并处理消息，是实现状态机复制的关键。

两者密切相关：实现全序广播的本质就是实现共识。

**⚙️ 核心协议对比：VSR、Paxos、Raft、Zab**

**一致性与共识的要点**

1. **节点可能崩溃，消息可能丢失** → 共识算法必须具备容错能力。

2. **容错的基础是消息不能丢失** → 需持久化日志、重试机制。

3. **VSR、Raft、Zab 的决策需稳定议员确认** → 强调领导者与多数派的角色。

4. **Paxos 的决策更灵活** → 只要满足 2M 的条件（多数派），任何节点可提议。

##### 主从复制与共识

在主从架构中，**所有写操作由主节点处理**，然后同步到从节点。

表面上看，主节点决定一切，似乎不需要共识。

但实际上，为了保证系统的**一致性与可靠性**，共识机制仍然不可或缺。

1. **主节点的决策是否可靠？**

* 如果主节点出现故障或行为异常（如脑裂），可能导致数据不一致。

* 因此，系统需要一种机制来验证主节点的决策是否被多数节点认可。

- **主节点的选举过程**

* 当主节点失效时，系统必须选出新的主节点。

* 这个过程本质上就是一个**共识问题**：多个节点必须就“谁是新主”达成一致。

* 常见协议如 Raft、Zab 都将主节点选举作为核心部分。

- **复制策略与一致性保障**

* 主节点在写入数据后，需将变更同步到从节点。

* 若同步机制不具备确认机制（如 ACK），可能导致数据丢失或不一致。

* 共识协议可以确保写入在多数节点上达成一致后才算成功。

##### Epoch和Quorum

1. Epoch（世代编号）

* **Epoch Number / Term Number / Ballot Number**：

  * 都是用于标识某一轮共识或领导者任期的编号。

  * 在 Raft 中称为 Term，在 Paxos 中称为 Ballot。

* **作用**

  * 将系统时间切分为不重叠的世代（epoch）

  * 每个 epoch 内 Leader 唯一且确定

  * 单调递增，保证新的 Leader 一定“更新”于旧 Leader

- Leader 选举与确认流程

  1. **故障检测**

     * 若怀疑当前 Leader 失效，发起新一轮选举

  2. **新 Leader 产生**

     * 投票产生一个更高的 epoch 编号

     * 如果出现两个 Leader → 编号高者胜

  3. **Leader 决策前检查**

     * 必须确认没有比自己更新的 epoch Leader 存在

     * 自身“认为是 Leader”并不够，需要 Quorum 认可

- Quorum（法定人数）要求

* **定义**：一个足够大的节点集合，能在决策上代表整个集群

* **常见情况**：多数节点（N/2 + 1），但并非必须是简单多数

* **作用**：

  * Leader 确认自己合法性

  * 对提案进行投票表决

* **关键约束**：选举 Quorum 与提案投票 Quorum 必须有 **重叠节点**，确保安全性（防止两个不同 epoch 的决议同时被提交）

- 两轮投票

  1. **轮 1**：选举 Leader（产生 epoch）

  2. **轮 2**：Leader 提案投票（带 epoch 号）

  * 如果提案投票中未出现更高的 epoch，说明 Leader 没被取代 → 提案安全可提交

##### 共识的局限性

**共识的优势与代价**

**✅ 优势（前提背景）**

* **安全属性**：一致性（Consistency）、完整性（Integrity）、有效性（Validity）

* **容错能力**：多数节点存活即可继续服务

* **线性化保证**：通过全序关系广播（Total Order Broadcast）实现原子操作的线性化语义

**&#x20;限制与成本**

#### 成员与协调服务

**类型**：分布式键值存储（Distributed KV Store）/ 协调与配置服务（Coordination & Configuration Service）

**主要用途**：为分布式系统提供可靠的小规模元数据存储与一致性保证

**非目标**：存储大量业务数据（其数据量可全部放入内存，磁盘仅用于持久化）

1. **为什么需要共识**

2. **🔑 核心特性**

   1. **线性化原子操作 (Linearizable Atomic Ops)**

      * 例如：CAS（Compare-And-Set）实现分布式锁

      * 结合租约（lease）机制，防止客户端失效后锁长期占用

   2. **操作全序 (Total Order)**

      * 所有操作赋予单调递增的事务 ID (zxid) + 版本号

      * 支持 fencing token 防止进程暂停导致的锁冲突

   3. **故障检测 (Failure Detection)**

      * 会话与心跳机制（Session + Heartbeat）

      * 会话失效 → 自动删除临时节点 (ephemeral nodes) → 释放锁资源

   4. **变更通知 (Watch/Notification)**

      * 客户端可订阅节点变化（成员加入、离开、数据变化）

      * 避免频繁轮询

3. &#x20;依赖场景（生态中的作用）

很多分布式系统（HBase, Hadoop YARN, Kafka, OpenStack Nova 等）依赖 ZK/etcd 去解决：

* 集群成员管理（Membership Management）

* Leader 选举

* 分布式锁/租约

* 配置分发

* 元数据一致性

##### 节点任务分配

1. 核心场景

**单主故障切换（Leader Failover）**

* 多个实例中始终保持一个主节点

* 主挂掉时由其他节点接管

* 常见于主从复制数据库、作业调度器、有状态服务

**分区资源分配（Partition Assignment）**

* 多个分区（DB 分片、消息流、文件块、Actor 等）分配给不同节点

* 新节点加入 → 迁移部分分区实现动态负载均衡

* 节点离开/失效 → 其他节点接管其分区

- ZooKeeper 提供的能力支撑

- 架构上的取舍

* **外包协调**：应用不需要自己实现共识、全序广播、故障检测 → 降低复杂度

* **数据变化频率低**：例如 “分区7的主节点在 10.1.1.23” 这种信息，分钟/小时级更新即可

* **不适合高频实时状态**：如果是每秒数千到百万级变化（例如指标流、事件流），应考虑 BookKeeper、Kafka 这种流式或日志存储

##### 服务发现

1. 核心场景：服务发现的角色

* **目标**：让客户端动态找到可用服务的地址（IP/端口等）

* **挑战**：云环境中节点常常会频繁上下线，IP 动态变化，启动前很难提前知道彼此的位置

* **典型实现**：

  1. 节点启动时向 ZooKeeper / etcd / Consul 注册自身网络信息

  2. 其他组件通过查询注册表获取当前可用的服务地址

- 共识在服务发现中的取舍

> 现实中，DNS 的设计哲学是**可用性优先**：即使网络中断时依然尽可能返回缓存结果。这种策略适合多数服务发现场景。

* 🛠 共识系统与只读副本

- 某些共识系统支持 **只读缓存副本**（Read-only Replica）

  * 异步接收由投票节点达成的决议日志

  * 自身不参与投票

  * 适合提供不需要强一致的读服务（如非关键的服务发现查询）

##### 成员服务

1. **🧩 成员服务的核心概念**

   * 用于确定“当前集群有效成员”的系统组件

   * 常回答两个问题：

     1. 哪些节点被视为集群的一部分

     2. 这些节点目前是否处于“活跃”状态

2. **关键特点**

   1. **与故障检测绑定的共识**

      * 单纯的网络心跳 ≠ 可靠故障判断（因网络延迟不可避免）

      * 通过共识让所有存活节点**一致认定**某个节点是否失效

      * 即便存在误判，保证全体决策一致性仍是首要目标 （避免不同节点眼中的“集群成员列表”不一致）

   2. **典型应用场景**

      * **Leader 选举**：如选择编号最小的节点为主

      * **集群 reconfiguration**：添加、移除节点需要全体一致

      * **任务分配 / 决策投票**：前提是所有参与方对“谁在场”有相同认知

   3. **共识失败条件**

      * 若各节点对“当前集群成员集”存在分歧 → 共识过程无法继续

      * 因为协议参与方集合不一致会破坏安全性保证（投票多数派定义失效）

3. **📌 和之前讨论内容的联系**

   * **与协调服务的关系**

     * ZooKeeper / etcd 提供了内置的成员服务（Membership）能力

     * 这层能力是 Leader 选举、分区分配、锁服务的前提条件

   * **与 Epoch / Quorum 的关系**

     * Epoch 机制默认节点集合固定，一旦动态调整就需要成员服务介入

     * Quorum 的定义依赖于“有效成员集”的共识结果
