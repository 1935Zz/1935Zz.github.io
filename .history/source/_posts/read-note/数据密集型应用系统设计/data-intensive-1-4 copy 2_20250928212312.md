---
title: 数据密集型应用系统设计（1-4章）
date: 2025-09-28 20:39:32
categories:
   - [Read-Note, 数据密集型应用系统设计]
---



## 七、事务

![](/images/diagram-2.png)

将应用程序的多个读、写操作捆绑在一起成为一个逻辑操作单元。即事务中的所有读写是一个执行的整体，整个事务要么成功（提交）、要么失败（中止或回滚）。如果失败，应用程序可以安全地重试。这样，由于不需要担心部分失败的情况（无论出于何种原因），应用层的错误处理就变得简单很多。

> 事务不是一个天然存在的东西，它是被人为创造出来，目的是简化应用层的编程模型。有了事务，应用程序可以不用考虑某些内部潜在的错误以及复杂的并发性问题，这些都可以交给数据库来负责处理（我们称之为安全性保证）。

### **深入理解事务**

事务与可扩展性并不完全对立，也不是必备功能

**ACID 含义**

原子性（Atomicity），一致性（Consistency），隔离性（Isolation）与持久性（Durability）

与 ACID 对立，BASE，基本可用性（BasicallyAvailable），软状态（Softstate）和最终一致性（Eventualconsistency）

**原子性**

在出错时中止事务，并将部分完成的写人全部丢弃。如果事务已经中止，应用程序可以确定没有实质发生任何更改，所以可以安全地重试

**一致性**

对数据有特定的预期状态，任何数据更改必须满足这些状态约束（或者恒等条件）。例如，对于一个账单系统，账户的贷款余额应和借款余额保持平衡。如果某事务从一个有效的状态开始，并且事务中任何更新操作都没有违背约束，那么最后的结果依然符合有效状态。

本质上要求应用层来维护状态一致，应用程序有责任正确地定义事务来保持一致性

**隔离性**

多个客户端访问相同记录的并发问题

并发执行的多个事务相互隔离，它们不能互相交叉

**持久性**

保证一旦事务提交成功，即使存在硬件故障或数据库崩溃，事务所写入的任何数据也不会消失

**单对象与多对象事务操作**

原子性和隔离性主要针对客户端在同一事务中包含多个写操作时，数据库所提供的保证。单对象也同样适用。

**多对象事务的必要性**

关系数据模型，某一行是另一个外键。

文档数据模型，可能需要一次更新多个文档

二级索引数据库，需要同步更新索引

**处理错误与中止**

重试中止的事务虽然是一个简单有效的错误处理机制，但它并不完美

### 弱隔离级别

可串行化性能较低，为了不牺牲性能，更多倾向于采用较弱的隔离级别，它可以防止某些但并非全部的并发问题

#### **读-提交**

读数据库只能看到已成功提交的数据（防止脏读）

写数据库时只会覆盖已成功提交的数据（防止脏写）

防止看到部分更新，防止写操作回滚时看到被回滚的数据（实际这些数据不会提交）

对于是尚未提交事务的先前的写入，防止另一个事务提交覆盖先前的写入。通常通过推迟第二个写请求，直到前面的的事务提交

![](/images/image-34.png)

**实现读-提交**

通常采用行级锁来防止脏写

防止脏读，一种选择是使用相同的锁，所有试图读取该对象的事务必须先申请锁，事务完成后释放锁。但运行时间较长的写事务会导致许多只读的事务等待太长时间，这会严重影响只读事务的响应延迟

大多数据库方法：对于每个待更新的对象，数据库都会维护其旧值和当前持锁事务将要设置的新值两个版本。在事务提交之前，所有其他读操作都读取旧值；仅当写事务提交之后，才会切换到读取新值

#### **快照级别隔离与可重复读**

![读倾斜，对于读提交的问题，不可重复读](/images/image-33.png)

快照级别隔离，解决常见手段。其总体想法是，每个事务都从数据库的一致性快照中读取，事务一开始所看到是最近提交的数据，即使数据随后可能被另一个事务更改，但保证每个事务都只看到该特定时间点的旧数据。

**实现快照级别隔离**

写加锁防止脏写，但读取不加锁

实现：保留了对象多个不同的提交版本，这种技术因此也被称为多版本并发控制（Multi-VersionConcurrencyControl，MVCC）

PostgreSQL 实现方式（其它类似）：每行额外两个字段，created\_by 字段表示创建该行事务 ID，deleted\_by 字段表示删除该行事务 ID，更新内部转换为一个删除操作和一个创建操作

**快照可见性规则**

1. 事务开始时列出当前正在进行的事务，之后忽略这些事务的写入

2. 中止事务不可见

3. 较晚事务不可见

4. 除此之外的事务写入可见

**索引和快照级别隔离**

一种方案：索引直接指向对象的所有版本，然后想办法过滤对当前事务不可见的那些版本

另一种方案：追加式 B-tree，每个写入事务创建新的 B-treeroot，修改时创建新的修改副本，拷贝必要的内容，让上面的节点指向新创建的节点

#### **防止更新丢失**

对于 read-modify-write 过程，两个事务在同样的数据对象执行类似操作，由于隔离性，第二个写操作不包含第一个事务修改后的值，导致第一个事务修改至可能会丢失

解决方案：

**原子写操作**

数据库层

UPDATEcountersSETvalue=value+1WHEREkey='foo'并发安全

**显式加锁**

数据库不支持内置原子操作，应用程序显示锁定待更新对象，如 select...forupdate

**自动检测更新丢失**

原子操作和锁本质是让“读-修改-写回”的序列串行执行，另一种思路是先让他们并发执行，如果事务管理器检测到了更新丢失风险，则会中断当前事务，强制回退到安全的“读-修改-写回”方式

借助快照级别隔离可以高效执行检查，但不是所有数据库支持检测更新丢失

**原子比较和设置**

只有在上次读取的数据没有发生变化时才允许更新；如果已经发生了变化，则回退到“读-修改-写回”方式。

**冲突解决与复制**

多副本数据库，不同节点可能会并发修改数据，需采取额外措施防止丢失更新

#### **写倾斜与幻读**

设想这样一个例子：你正在开发一个应用程序来帮助医生管理医院的轮班。通常，医院会安排多个医生值班，医生也可以申请调整班次（例如他们自己生病了），但前提是确保至少一位医生还在该班次中值班。

现在情况是，Alice 和 Bob 是两位值班医生。两人碰巧都感到身体不适，因而都决定请假。不幸的是，他们几乎同一时刻点击了调班按钮。

每笔事务总是首先检查是否至少有两名医生目前在值班。如果是的话，则有一名医生可以安全里离开。由于数据库正在使用快照级别隔离，两个检查都返回有两名医生，所以两个事务都安全地进入到下一个阶段。接下来 Alice 更新自己的值班记录为离开，同样，Bob 也更新自己的记录。两个事务都成功提交，最后的结果却是没有任何医生在值班，显然这违背了至少一名医生值班的业务要求。

**定义写倾斜**

广义的更新丢失。如果两个事务读取相同的一组对象，更新其中一部分，不同的事务可能更新不同的对象，可能发生写倾斜，不同的事务如果更新同一个对象，可能发生脏写或更新丢失

**更多写倾斜的例子**

会议室预定系统，多人游戏，声明一个用户名，防止双重开支

**为何产生写倾斜**

写倾斜的规律

1. select 查询满足条件的行（例如，至少有两名医生正在值班，同一时刻房间没有预订，棋盘的某位置没有出现数字，用户名还没有被占用，账户里还有余额等）

2. 根据查询的结果，应用层决定是否继续操作

3. 如果继续，发起写入

写入会改变步骤 2 做出决定的前提条件，即提交写入后重复执行步骤一的 select 查询，会返回完全不同的结果

这种在一个事务中的写入改变了另一个事务查询结果的现象，称为幻读

### 串行化

* 隔离级别难以理解，不同数据库实现不一致

* 检查应用层代码，很难判断它在特定隔离级别下是否安全

* 缺乏工具检测竞争状况

解决方案：串行化，即使事务可能会并行执行，最终结果与每次一个即串行执行结果相同，防止所有竞争条件

#### 实际串行执行

在一个线程上按顺序方式每次只执行一个事务

**采用存储过程封装事务**

![](/images/image-37.png)

**分区**

串行执行所有事务使得并发控制更加简单，但是数据库的吞吐量被限制在单机单个 CPU 核。虽然只读事务可以在单独的快照上执行，但是对于那些高写入需求的应用程序，单线程事务处理很容易成为严重的瓶颈。

对数据分区，每个事务只在单个分区内读写数据，每个分区都可以有自己的事务处理线程且独立运行

跨分区事务需要额外的协调，性能要低很多

**串行执行小结**

* 事务必须简短而高效，否则一个缓慢的事务会影响到所有其他事务的执行性能。

* 仅限于活动数据集完全可以加载到内存的场景。有些很少访问的数据可能会被移到磁盘，但万一单线程事务需要访问它，就会严重拖累性能。

* 写入吞吐量必须足够低，才能在单个 CPU 核上处理；否则就需要采用分区，最好没有跨分区事务。

* 跨分区事务虽然也可以支持，但是占比必须很小

#### 两阶段加锁

two-phaselocking,2PL

读写锁

第一阶段事务执行前获取锁，第二阶段事务结束释放锁

死锁，数据库系统自动检测

缺点：性能下降，访问延迟不确定性，可能因为锁等待很长时间

**谓词锁**

防止写倾斜和幻读（如会议室预定场景）

谓词锁：不属于某个特定对象，作用于满足某些搜索条件的所有查询对象

谓词锁会限制如下访问：

* 如果事务 A 想要读取某些满足匹配条件的对象，例如采用 SELECT 查询，它必须以共享模式获得查询条件的谓词锁。如果另一个事务 B 正持有任何一个匹配对象的互斥锁，那么 A 必须等到 B 释放锁之后才能继续执行查询

* 如果事务 A 想要插入、更新或删除任何对象，则必须首先检查所有旧值和新值是否与现有的任何谓词锁匹配（即冲突）。如果事务 B 持有这样的谓词锁，那么 A 必须等到 B 完成提交（或中止）后才能继续

**索引区间锁**

谓词锁性能差，大部分 2PL 使用索引区间锁（next-keylocking），本质是对谓词锁的简化或近似



#### 可串⾏化的快照隔离

提供了完整的可串⾏性保证，⽽性能相⽐于快照隔离损失很⼩

基于快照隔离，读取基于快照，同时增加了相关算法检测写入冲突来决定是否中止事务

事务基于某些前提条件成立而执行，如果提交时条件发生变化，则需中止

数据库需检测事务是否会修改其他事物的查询结果，具体分以下 2 种情况

1. 是否读取的是一个过期的 MVCC 对象

事务读取时是其它某个事务未提交的对象，提交时该对象已经被其它某个事务提交，

* 写入是否影响即将完成的读取

![](</images/截屏2025-06-24 17.43.41.png>)

事务 42 的写影响了事务 43 的读取（42 先于 43 提交）



## 八、分布式系统挑战

![](/images/diagram-3.png)

与单节点系统差异显著，会出现各种问题

核心：构建可靠系统

认清分布式系统状态本质->评估所发生的各种故障

### 故障与部分失效

单节点运行具有确定性，当硬件正常时，主要问题是 bug，且相同的操作通常会产生相同的结果

分布式系统会出现部分失效，即一部分工作正常，一部分出现故障，难点在于部分失效的不确定：如有时网络正常，有时失败

#### 云计算和超算

构建大规模计算系统思路

1. 高性能计算，包含成千上万个 CPU 的超级计算机构成一个庞大的集群，通常用于计算密集型科学计算

2. 云计算

不同构建方式处理错误方法也不同。高性能计算通常会对任务保存快照，出现故障停止整个集群，修复后从最近快照执行（更像是一个单节点系统）

本书重点是基于互联网的服务，与 HPC 不同

1. 大部分是在线服务

2. HPC 通常采用专用的硬件，每个节点可靠性高，节点间主要通过共享内存或远程内存直接访问等技术通信。云计算节点多由通用机器构建，单节点成本低廉

3. 大型数据中心通常基于 IP 和以太网，HPC 通常基于特定的网络拓扑结构

4. 可容忍失败节点，让整体继续工作

5. 数据中心分布全球部署（让用户访问地理靠近的数据中心，降低延迟），HPC 通常假设节点位置靠近

必然面临部分失效，依靠软件系统提供容错机制，即在不可靠的组件上构建可靠的系统

### 不可靠的网络

主要关注分布式无共享系统：通过网络连接的多个节点，网络是跨节点通信唯一路径

互联网大多数据中心内部网络基于异步网络，节点通过网络发送消息到另一个节点，但网络不保证什么时候到达及是否到达。发送到等待响应过程中，可能出错的事情

1. 请求已丢失

2. 请求在某个队列中等待，无法马上发送

3. 远程接收节点已失效（如崩溃或关机）

4. 远程接收节点暂时无法响应

5. 远程接收节点已经完成请求处理，但回复在网络中丢失

6. 远程接收节点已经完成请求处理，回复被延迟处理

![](</images/截屏2025-06-25 15.51.37.png>)

#### 检测故障

需要自动检测节点失效

#### 超时和无限的延迟

延迟理论无限大，大多数系统无法保障数据包传输的延迟和处理数据包的时间

超时时间不好设置

**网络拥塞和排队**

延迟变化根源往往在于排队

1. 网络交换机处理大量数据包时的排队

2. 目标机器 cpu 均繁忙时，会被操作系统处理排队

3. 虚拟化环境 cpu 切换虚拟机导致的排队

4. tcp 流量控制的排队，以及 tcp 重传引入的延迟

### 不可靠的时钟

许多场景依赖时钟，包括判断请求超时，服务 p99，用户浏览时间，文章发表时间，缓存过期时间……

NTP（Network Time Protocol）网络时间协议：可以根据一组专门的时间服务器调整本地时间



## 九、一致性与共识

![](/images/diagram-4.png)

类比事务的抽象可以让应用程序可以假装没有崩溃（原⼦性），没有与其他⼈并发访问数据库（隔离性），且存储设备是完全可靠的（持久性），尝试建立可以让分布式系统应用忽略内部各种问题的抽象机制

**重要抽象之一：共识**

分布式系统可提供的若干保证和抽象机制  -->  解决共识问题的相关算法



### 一致性保证

最终一致性：停止更新数据库，等待一段时间后最终所有读请求会返回相同的内容。即不一致现象是暂时的，最终会达到一致

更强的一致性模型，意味着更多代价，如性能降低或容错性降低



### 可线性化

![非线性化例子](</images/截屏2025-06-27 14.35.10.png>)

可线性化基本思想：**使系统看起来好像只有一个数据副本**

![一个重要约束](</images/截屏2025-06-27 14.56.19.png>)

![](</images/截屏2025-06-27 15.01.41.png>)

CAP 理论，在出现网络故障时，一致性（线性化）和可用性无法兼具

线性化和性能之间的取舍



### 顺序保证

线性化的操作按某种顺序执行（图 9-4）

一些顺序要求的场景：

* 主从复制时，主节点确定复制日志写入顺序

* 可串行化确保的事务执行结果与按照某种顺序方式执行一样。

#### 顺序与因果关系

顺序的作用：保障因果关系

要求因果关系的一些场景：

* 图 5-5，先有问题的答案，再有问题的本身，违背了因果关系

* 图 5-9，需要先有创建数据行，后有更新

* 第 5 章“检测并发写”，两个操作 A 和 B，三种关系，A 发生在 B 之前，B 发生在 A 之前，A 和 B 并发，前两者都是因果关系

* 事务快照隔离，事务从一致性快照读取，一致性即与因果关系一直，如果快照包含了答案，那么它也应该包含所提的问题

* 事务写倾斜，图 7-8 调班动作的因果关系取决于当前值班人，可序列化的快照隔离（可串行化的快照隔离）主要通过跟踪事务之间的因果依赖关系来达到检测写倾斜目的

**因果顺序并⾮全序**

全序：支持任意两个元素的比较

因果：某两个事件的关系，一个发生在另一个前

#### 序列号排序

使用序列号排序事件

存在唯一主节点，可以让主节点为每个操作递增某个计数器，从而为复制日志中的每个操作赋值一个单增的序列号

**非因果序列发生器**

不存在唯一主节点，产生序列号较复杂

三个思路

1. 每个节点独立产生自己的一组序列号，比如 A 产生奇数，B 产生偶数，或者 A 产生 1-1000，B 产生 1000-2000

2. 时间戳信息（物理时钟）附加到操作上

问题：产生的序列号与因果关系不严格一致

**Lamport 时间戳**

值对（计数器，节点 ID）

![](/images/image-36.png)

最大计数器值嵌入到每一个请求中，如果发现收到的响应或请求中的最大值大于自身的技术器值，则把自己的计数器修改为该最大值

#### 全序关系广播

比喻：想象一个聊天群，里面有多个成员。全序关系广播要保证：无论消息从哪个成员发出，也无论每个成员在什么时间收到消息，所有成员看到的聊天记录顺序必须完全一致。

全序关系广播是一种消息传递协议，它确保在分布式系统的所有节点上，消息的传递顺序是完全一致的。它需要满足以下**两个核心属性**：

1. 可靠广播：如果一个正确的（非故障的）节点广播了一条消息，那么所有正确的节点最终都会收到这条消息。（不会丢失消息）

2. 全序传递：任何两个正确的节点，它们收到所有消息的顺序都是完全相同的。

**为什么需要它？**

在分布式系统（如数据库、状态机）中，多个副本需要保持完全一致的状态。实现这一目标最有效的方法就是让所有副本都以相同的顺序执行相同的操作指令。全序广播正是用来保证所有副本收到操作指令的顺序一致的机制。

实现技术：

常见的共识算法，如 Paxos 和 Raft，其核心功能之一就是实现全序关系广播。在 Raft 中，领导者（Leader）负责为所有客户端请求（即日志条目）确定一个唯一的顺序，并将这个顺序复制到所有追随者（Follower）节点上。



全序关系广播是实现线性化存储的一种强大工具。

* 如果你有一个全序关系广播系统，你可以很容易地构建一个线性化的存储。

  * 方法：将所有写操作（有时也包括读操作）通过全序广播发送给所有副本。每个副本按照广播确定的全局唯一顺序来依次执行这些操作。由于所有副本以相同顺序执行相同操作，它们的最终状态会保持一致，并且整个系统对外表现为线性化。



### 分布式事务与共识

需要达成一致的场景

* 主节点选举

* 原子事务提交

#### 原子提交和2PC（两阶段提交）

![](/images/image-32.png)

![](/images/image-31.png)

#### 实践中的分布式事务

##### **Exactly-once 消息处理**

* 在分布式系统中，消息可能因为网络抖动或服务故障而被**重复发送**。

* 如果处理逻辑不是幂等的（idempotent），重复处理会导致数据错误。

* Exactly-once 的目标是：**无论消息被发送多少次，处理结果都只发生一次且正确。**

**⚙️ 技术策略：如何实现 Exactly-once？**

1. **使用唯一消息 ID（Message ID）**

   * 每条消息附带一个唯一标识符。

   * 消费者记录已处理的 ID，避免重复处理。

2. **幂等操作（Idempotent Operations）**

   * 设计处理逻辑，使得重复执行不会改变最终结果。

   * 例如：数据库插入使用 UPSERT、更新使用 SET 而非 INCREMENT。

3. **事务性处理（Transactional Processing）**

   * 将消息消费与状态更新放入同一个事务中。

   * 如果事务失败，消息不会被标记为已处理。

4. **去重存储（Deduplication Store）**

   * 使用 Redis、数据库或日志系统记录已处理消息 ID。

   * 注意存储的 TTL 和性能影响。

**📌 实践挑战与权衡**

* **性能 vs. 精度**：Exactly-once 通常比 At-least-once 慢，因为需要额外的存储和检查。

* **状态一致性**：消费者必须维护处理状态，可能引入复杂的状态同步问题。

* **系统设计**：Kafka、RabbitMQ 等消息系统本身不保证 Exactly-once，需要应用层配合。

##### **XA交易**

**XA（eXtended Architecture）** 是由 X/Open 标准制定的分布式事务处理协议。

它用于在多个数据库或消息系统之间协调事务，确保满足 **ACID**（原子性、一致性、隔离性、持久性）属性。

支持 XA 的系统和工具

* 常见数据库：PostgreSQL、MySQL、DB2、SQL Server、Oracle 等。

* 消息中间件：ActiveMQ、HornetQ、MSMQ、IBM MQ 等

XA 使用 **两阶段提交协议（2PC）** 来确保所有参与者一致提交或回滚事务。

事务管理器负责协调各个资源的准备和提交过程。



##### **协调者故障中恢复**

当系统组件发生故障时，恢复不仅仅是重启，而是要**恢复到一致的状态**。

为了实现这一目标，系统需要依赖**日志记录**和**协调机制**。

**📚 关键概念拆解**

1. **日志的重要性**：

   * 每个组件记录自己的操作日志。

   * 恢复时通过重放日志来恢复状态。

   * 但日志可能不完整或顺序不一致，需协调。

2. **一致性判断的挑战**：

   * 系统需要判断哪些操作已经完成，哪些未完成。

   * 如果没有协调机制，可能会出现状态不一致或数据丢失。

3. **协调机制的作用**：

   * 协调多个组件的日志与状态，确保恢复后系统整体一致。

   * 类似于分布式事务中的两阶段提交（2PC）或更复杂的共识协议（如 Paxos、Raft）。

**实践中的恢复策略**

* **检查点（Checkpoint）**：定期保存系统状态，减少恢复时的复杂度。

* **幂等性设计**：确保操作可以重复执行而不会造成副作用。

* **恢复流程的验证**：系统设计中必须考虑如何验证恢复的正确性。

##### 分布式事务限制

* **分布式事务（如 XA）虽然能提供强一致性，但代价高昂**。

* 在大规模系统中，XA 的性能瓶颈和复杂性使其难以落地。

* 许多现代系统（如 HDFS、HBase）选择放弃 XA，采用更灵活的方式处理一致性。

**XA 的问题**：

* 实现复杂，依赖底层资源管理器的支持。

* 性能开销大，尤其在高并发场景下。

* 容错能力弱，一旦协调者故障，可能导致事务阻塞。

**2PC 的局限性**：

* **阻塞问题**：参与者在等待协调者指令时无法继续处理其他事务。

* **单点故障**：协调者崩溃后，系统可能陷入不确定状态。

* **缺乏自动恢复机制**：无法应对网络分区或节点重启后的状态恢复。

**一致性 vs 可用性**：

* 在 CAP 理论下，强一致性往往牺牲了可用性。

* 某些系统选择最终一致性或幂等操作来提升可用性。



#### 支持容错的共识

* 在分布式系统中，多个节点需要就某个事实达成一致（consensus），即使存在网络延迟、节点故障或恶意行为。

* 共识协议的目标是确保系统在不可靠环境下仍能做出一致决策。

**📐 共识协议的基本性质**

1. **一致性（Uniform Agreement）**

   * 所有非故障节点最终必须达成相同的决策。

2. **有效性（Validity）**

   * 如果某个节点提出了一个值，最终决定的值必须是某个节点曾提出过的。

3. **终止性（Termination）**

   * 所有非故障节点最终都必须做出决定，不能无限等待。

##### 共识算法与全序广播

* **共识算法（Consensus Algorithm）**：用于在多个节点之间就某个值达成一致，确保系统状态一致性。

* **全序广播（Total Order Broadcast）**：确保所有节点以相同顺序接收并处理消息，是实现状态机复制的关键。

两者密切相关：实现全序广播的本质就是实现共识。

**⚙️ 核心协议对比：VSR、Paxos、Raft、Zab**

**一致性与共识的要点**

1. **节点可能崩溃，消息可能丢失** → 共识算法必须具备容错能力。

2. **容错的基础是消息不能丢失** → 需持久化日志、重试机制。

3. **VSR、Raft、Zab 的决策需稳定议员确认** → 强调领导者与多数派的角色。

4. **Paxos 的决策更灵活** → 只要满足 2M 的条件（多数派），任何节点可提议。

##### 主从复制与共识

在主从架构中，**所有写操作由主节点处理**，然后同步到从节点。

表面上看，主节点决定一切，似乎不需要共识。

但实际上，为了保证系统的**一致性与可靠性**，共识机制仍然不可或缺。

1. **主节点的决策是否可靠？**

* 如果主节点出现故障或行为异常（如脑裂），可能导致数据不一致。

* 因此，系统需要一种机制来验证主节点的决策是否被多数节点认可。

- **主节点的选举过程**

* 当主节点失效时，系统必须选出新的主节点。

* 这个过程本质上就是一个**共识问题**：多个节点必须就“谁是新主”达成一致。

* 常见协议如 Raft、Zab 都将主节点选举作为核心部分。

- **复制策略与一致性保障**

* 主节点在写入数据后，需将变更同步到从节点。

* 若同步机制不具备确认机制（如 ACK），可能导致数据丢失或不一致。

* 共识协议可以确保写入在多数节点上达成一致后才算成功。

##### Epoch和Quorum

1. Epoch（世代编号）

* **Epoch Number / Term Number / Ballot Number**：

  * 都是用于标识某一轮共识或领导者任期的编号。

  * 在 Raft 中称为 Term，在 Paxos 中称为 Ballot。

* **作用**

  * 将系统时间切分为不重叠的世代（epoch）

  * 每个 epoch 内 Leader 唯一且确定

  * 单调递增，保证新的 Leader 一定“更新”于旧 Leader

- Leader 选举与确认流程

  1. **故障检测**

     * 若怀疑当前 Leader 失效，发起新一轮选举

  2. **新 Leader 产生**

     * 投票产生一个更高的 epoch 编号

     * 如果出现两个 Leader → 编号高者胜

  3. **Leader 决策前检查**

     * 必须确认没有比自己更新的 epoch Leader 存在

     * 自身“认为是 Leader”并不够，需要 Quorum 认可

- Quorum（法定人数）要求

* **定义**：一个足够大的节点集合，能在决策上代表整个集群

* **常见情况**：多数节点（N/2 + 1），但并非必须是简单多数

* **作用**：

  * Leader 确认自己合法性

  * 对提案进行投票表决

* **关键约束**：选举 Quorum 与提案投票 Quorum 必须有 **重叠节点**，确保安全性（防止两个不同 epoch 的决议同时被提交）

- 两轮投票

  1. **轮 1**：选举 Leader（产生 epoch）

  2. **轮 2**：Leader 提案投票（带 epoch 号）

  * 如果提案投票中未出现更高的 epoch，说明 Leader 没被取代 → 提案安全可提交

##### 共识的局限性

**共识的优势与代价**

**✅ 优势（前提背景）**

* **安全属性**：一致性（Consistency）、完整性（Integrity）、有效性（Validity）

* **容错能力**：多数节点存活即可继续服务

* **线性化保证**：通过全序关系广播（Total Order Broadcast）实现原子操作的线性化语义

**&#x20;限制与成本**

#### 成员与协调服务

**类型**：分布式键值存储（Distributed KV Store）/ 协调与配置服务（Coordination & Configuration Service）

**主要用途**：为分布式系统提供可靠的小规模元数据存储与一致性保证

**非目标**：存储大量业务数据（其数据量可全部放入内存，磁盘仅用于持久化）

1. **为什么需要共识**

2. **🔑 核心特性**

   1. **线性化原子操作 (Linearizable Atomic Ops)**

      * 例如：CAS（Compare-And-Set）实现分布式锁

      * 结合租约（lease）机制，防止客户端失效后锁长期占用

   2. **操作全序 (Total Order)**

      * 所有操作赋予单调递增的事务 ID (zxid) + 版本号

      * 支持 fencing token 防止进程暂停导致的锁冲突

   3. **故障检测 (Failure Detection)**

      * 会话与心跳机制（Session + Heartbeat）

      * 会话失效 → 自动删除临时节点 (ephemeral nodes) → 释放锁资源

   4. **变更通知 (Watch/Notification)**

      * 客户端可订阅节点变化（成员加入、离开、数据变化）

      * 避免频繁轮询

3. &#x20;依赖场景（生态中的作用）

很多分布式系统（HBase, Hadoop YARN, Kafka, OpenStack Nova 等）依赖 ZK/etcd 去解决：

* 集群成员管理（Membership Management）

* Leader 选举

* 分布式锁/租约

* 配置分发

* 元数据一致性

##### 节点任务分配

1. 核心场景

**单主故障切换（Leader Failover）**

* 多个实例中始终保持一个主节点

* 主挂掉时由其他节点接管

* 常见于主从复制数据库、作业调度器、有状态服务

**分区资源分配（Partition Assignment）**

* 多个分区（DB 分片、消息流、文件块、Actor 等）分配给不同节点

* 新节点加入 → 迁移部分分区实现动态负载均衡

* 节点离开/失效 → 其他节点接管其分区

- ZooKeeper 提供的能力支撑

- 架构上的取舍

* **外包协调**：应用不需要自己实现共识、全序广播、故障检测 → 降低复杂度

* **数据变化频率低**：例如 “分区7的主节点在 10.1.1.23” 这种信息，分钟/小时级更新即可

* **不适合高频实时状态**：如果是每秒数千到百万级变化（例如指标流、事件流），应考虑 BookKeeper、Kafka 这种流式或日志存储

##### 服务发现

1. 核心场景：服务发现的角色

* **目标**：让客户端动态找到可用服务的地址（IP/端口等）

* **挑战**：云环境中节点常常会频繁上下线，IP 动态变化，启动前很难提前知道彼此的位置

* **典型实现**：

  1. 节点启动时向 ZooKeeper / etcd / Consul 注册自身网络信息

  2. 其他组件通过查询注册表获取当前可用的服务地址

- 共识在服务发现中的取舍

> 现实中，DNS 的设计哲学是**可用性优先**：即使网络中断时依然尽可能返回缓存结果。这种策略适合多数服务发现场景。

* 🛠 共识系统与只读副本

- 某些共识系统支持 **只读缓存副本**（Read-only Replica）

  * 异步接收由投票节点达成的决议日志

  * 自身不参与投票

  * 适合提供不需要强一致的读服务（如非关键的服务发现查询）

##### 成员服务

1. **🧩 成员服务的核心概念**

   * 用于确定“当前集群有效成员”的系统组件

   * 常回答两个问题：

     1. 哪些节点被视为集群的一部分

     2. 这些节点目前是否处于“活跃”状态

2. **关键特点**

   1. **与故障检测绑定的共识**

      * 单纯的网络心跳 ≠ 可靠故障判断（因网络延迟不可避免）

      * 通过共识让所有存活节点**一致认定**某个节点是否失效

      * 即便存在误判，保证全体决策一致性仍是首要目标 （避免不同节点眼中的“集群成员列表”不一致）

   2. **典型应用场景**

      * **Leader 选举**：如选择编号最小的节点为主

      * **集群 reconfiguration**：添加、移除节点需要全体一致

      * **任务分配 / 决策投票**：前提是所有参与方对“谁在场”有相同认知

   3. **共识失败条件**

      * 若各节点对“当前集群成员集”存在分歧 → 共识过程无法继续

      * 因为协议参与方集合不一致会破坏安全性保证（投票多数派定义失效）

3. **📌 和之前讨论内容的联系**

   * **与协调服务的关系**

     * ZooKeeper / etcd 提供了内置的成员服务（Membership）能力

     * 这层能力是 Leader 选举、分区分配、锁服务的前提条件

   * **与 Epoch / Quorum 的关系**

     * Epoch 机制默认节点集合固定，一旦动态调整就需要成员服务介入

     * Quorum 的定义依赖于“有效成员集”的共识结果
