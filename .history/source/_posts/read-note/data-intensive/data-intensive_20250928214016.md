---
title: 数据密集型应用系统设计（1-4章）
date: 2025-09-28 20:39:32
categories:
   - [Read-Note, 数据密集型应用系统设计]
---

对于⼀个应⽤系统，如果“数据”是其成败决定性因素，包括数据的规模、数据的复杂度或者数据产⽣与变化的速率等，我们就可以称为“数据密集型应⽤系统”，与之对应的是计算密集型（Compute-Intensive），CPU 主频往往是后者最⼤的制约瓶颈

# 数据系统基础

## 一、可靠、可扩展与可维护的应用系统

**可靠性：当出现意外情况如硬件、软件故障、人为失误等，系统应可以继续正常运转：虽然性能可能有所降低，但确保功能正确。**

如何保障可靠性

* 最小出错方式设计系统，如精心设计的抽象层、API 以及管理界面

* 分离易出错地方、容易引发故障的接口，如提供一个功能齐全但非生产用的沙箱环境，使人们可以放心的尝试、体验，包括导入真实的数据，万一出现问题，不会影响真实用户

* 充分测试

* 快速恢复机制，滚动发布，数据校验

* 清晰监控



**可扩展性：随着规模的增长，例如数据量、流量或复杂性，系统应以合理的方式来匹配这种增长。描述系统应对负载增加能力**

描述负载：如 qps，数据库写入比例，聊天室的同时话动用户数量，缓存命中率等，平均值或峰值

Twitter 的例子，每个用户关注者的分布情况（还可以结合用户使用 Twitter 频率情况进行加权）是该案例可扩展的关键负载参数

两个业务场景：发布 tweet 消息，用户可以快速推送新消息到所有的关注者；主页时间线浏览，查看关注对象的最新消息

两个方案

![](data-intensive/image-10.png)

![](data-intensive/image-9.png)

描述性能：离线-吞吐量，在线-响应时间，sli，slo，sla，avg，p50，p99......

应对负载增加：垂直扩展，水平扩展，分布式系统



**可维护性：随着时间的推移，许多新的人员参与到系统开发和运维，以维护现有功能或适配新场景等，系统都应高效运转**

1. 可运维性：方便运营团队来保持系统平稳运行。良好的可操作性意味着对系统健康状况有良好的可观测性和有效的管理方法

2. 简单性：简化系统复杂性，使新工程师能够轻松理解系统

**复杂性的表现方式：**&#x72B6;态空间的膨胀，模块紧耦合，令人纠结的相互依赖关系，不一致的命名和术语，为了性能而采取的特殊处理，另解决某特定问题而引人的特殊框架

**消除复杂性的手段之一：抽象。**&#x4E00;个好的设计抽象可以隐藏大量的实现细节，并对外提供干净、易懂的接口。一个好的设计抽象可用于各种不同的应用程序。这样，复用远比多次重复实现更有效率；另一方面，也带来更高质量的软件，而质量过硬的抽象组件所带来的好处，可以使运行其上的所有应用轻松获益

* 可演化性：后续工程师能够轻松地对系统进行改进，并根据需求变化将其适配到非典型场景，也称为可延伸性、易修改性或可塑性



## 二、数据模型与查询语言

数据的表示，由上而下

1. 现实世界建模，通过对象、数据结构、api

2. 通用数据模型（例如 JSON 或 XML 文档、关系数据库中的表或图模型）

3. 内存、磁盘、网络的字节格式（数据库工程师）

4. 电流，光脉冲（硬件工程师）

**关系模型和文档模型**

IMS（用于 IBM），层次模型，树状，类似文档数据库的 json 模型，很好支持一对多，较难支持多对多

出现关系模型（后来的 sql）和网络模型

网络模型：由 ConferenceonDataSystemLanguages（CODASYL）标准化，层次模型的推广，每个记录有多个父节点，由于相同记录有多条不同路径，需要手动选择路径，并且数据模型更改困难，需要大量手写代码

**关系模型**：定义所有数据格式，关系即元组的集合，没有复杂的嵌套结构，也没有复杂的访问路径，“访问路径“由查询优化器自动生成

之后出现 NoSQL，包括文档数据库和图数据库

**文档模型**：某种方式层次模型，目标用例是数据来自于自包含文档，且一个文档与其他文档之间的关联很少

对于多对一和多对多，关系模型和文档模型都由唯一标识符引用（外键，文档引用）

**数据查询语言**

命令型明确指出执行步骤

声明型指定数据模式和条件，隐藏了底层实现步骤和引擎实现细节，在数据库中好于命令式

MapReduce，两者都不是，底层的编程模型，计算集群分布执行

**图状数据模型**

针对相反的场景，目标用例是所有数据都可能会互相关联

属性图：顶点，边，可以看做两个关系表，一个顶点表，一个边表

cypher 查询语言，声明式

![](data-intensive/image-7.png)

三元存储

（主体，key，value），（主体，边 label，客体）

SPARQL：RDF 数据模型的三元存储查询语言

![](data-intensive/image-6.png)



> 图模型和网络模型比较
>
> 1. 网络模型指定了嵌套关系，限制了嵌套，图模型没有限制，任意顶点都可以连向其它顶点
>
> 2. 网络模型获取特定记录只能遍历其中一条访问路径，图模型可以通过顶点唯一 id 获取顶点，也可以索引查找某个顶点
>
> 3. 网络模型记录的自己录是有序集合，图模型顶点和边无序
>
> 4. 网络模型所有查询都是命令式，难以编写，易被模式变更破坏，图模型可以用命令式代码，也可以用声明式语言，如 Cypher 和 SPARQL

Datalog：谓语（主体，客体）的表达方式

![](data-intensive/image-11.png)

定义新谓语的规则，从数据或其他规则派生而来，每次完成一小块，派生出复杂的查询

## 三、数据存储与检索

### **数据结构**

对于 kv 存储，最简单，存储到一个文件中，每行记录一对 key，value。读效率低，但写效率快

索引：基于原始数据派生而来的额外数据结构，能够加速查询，通常会降低写的速度

**哈希索引**

如果是追加式文件，可以用一个 hashmap，保存到内存中，每个 key 映射到数据文件中在磁盘上的字节偏移量（Bitcask 存储引擎）

为了避免用尽磁盘空间，可以将日志分解成多个小段，每个小段维护一个 hashmap，同时可以对段压缩及合并（丢弃重复的键，保留最新的键的更新，可以用后台线程）

考虑额外的几个问题

1. 文件格式，csv 非最佳，更好的是采用二进制格式（占用空间更小）

2. 删除记录，对于删除场景，需要在文件中追加一个删除标记，合并时发现标记，则丢弃删除键

3. 崩溃恢复，数据库重启会使内存中的 hashmap 丢失，可以重新读取文件构建 hash，但较慢，可以将每个段的 hashmap 快照存储在磁盘上，以便重启后直接加载到内存中

4. 并发控制，写入有先后顺序，通常只有一个写线程，多个读线程

追加日志优势：

1. 顺序写，比随机写快（磁盘特性）

2. 段文件追加及不可变特性，并发和崩溃恢复简单，例如不会出现重写崩溃的时候新旧值混合

3. 合并旧段避免了文件碎片化

哈希索引局限：

1. 必须全部放入内存，不适合大量的键，放入磁盘性能差（随机访问），以及哈希冲突

2. 区间查询效率低

**SSTables 和 LSM-Tree**

SSTable：排序字符串表，要求文件格式 kv 按 key 排序，每个键在每个段文件只出现一次

相比哈希索引优点

1. 合并高效，并发读取多个段，比较每个段第一个键，取最小的拷贝到输出文件

2. 查找无需保存所有键索引，保存 A 和 B 的索引，可以索引查找 A 到 B 之间所有 key，区间查找高效

SSTable 构建和维护。可以采用红黑树或 AVL 树作为内存排序

* 分块存储，可压缩

写入立即追加到日志，用于崩溃恢复

应用：leveldb，rocksdb，Cassandra

LSM-Tree（Log-StructuredMerge-Tree）：日志结构合并树，基于合并和压缩排序文件原理

lucene，全文搜索索引引擎，kv 结构，key 是单词，value 是 id 列表，类 SSTable 排序文件保存映射关系

优化

* 布隆过滤器判断 key 不存在

* SSTables 压缩和合并是的顺序和实际，大小分级和分层存储

**B-trees**

常用于关系数据库

WAL 崩溃恢复，锁控制并发

![](data-intensive/diagram.png)

**其它索引结构**

二级索引，用于执行联结操作

聚集索引（在索引中直接保存行数据）

非聚集索引（仅存储索引中的数据的引用，行存放在堆文件），存放多个二级索引避免了复制数据

覆盖索引（索引中保存一些表的列值）

### 事务处理与分析处理

![](data-intensive/image-4.png)

放弃用 OLTP 用于分析，用单独的数据库进行分析

**数据仓库**

不影响 OLTP，从 OLTP 提取数据，分析友好

![提取-转换-加载(Extract-Transform-Load,ETL)](data-intensive/image-3.png)

事实表：星型模式的中心，每一行表示在特定时间发生的事件，数据量大，列数大

由于每次只读取一行中的少数几列，面向行的存储引擎仍需要将所有行从磁盘加载到内存中，效率较低

需要在大量行顺序扫描，索引重要性降低，更重要的是紧凑编码数据，减少磁盘读取数据量，出现列存储

### 列存储

适用于 OLAP 系统

列存储：不要将一行中的所有值存储在一起，而是将每列中的所有值存储在一起

列压缩：对每列的值序列压缩，可以采用位图编码，n 个不同的列值，用 n 个位图，每一位（0/1）表示该行的值是否是对应的列值

写操作：LSM-tree，先写入内存存储区（面向行或面向列皆可），添加到已排序结构中，再准备写入磁盘，累积足够写入后与磁盘列文件合并，写入新文件（Vertica 实现）

物化聚合：缓存常使用的一些技术或综合

一种方式：物化视图（标准视图 in 关系数据库），是查询结果的实际副本，被写到磁盘，随底层数据变化而变化，OLTP 应用较少，影响写入性能

物化视图的特殊情况：数据立方体（OLAP 立方体）

![二维的例子，也可能有多维](data-intensive/image-8.png)

优点：某些查询会很快

缺点：缺乏灵活性，只对特定情况查询有效



## 四、数据编码与演化

> 向后兼容较新的代码可以读取由旧代码编写的数据。
>
> 向前兼容较旧的代码可以读取由新代码编写的数据

### 数据编码格式

两种表示

* 在内存中，数据保存在对象、结构体、列表、数组、哈希表和树等结构中。这些数据结构针对 CPU 的高效访问和操作进行了优化（通常使用指针）。

* 将数据写入文件或通过网络发送时，必须将其编码为某种自包含的字节序列（例如 JSON 文档）

从内存中的表示到字节序列的转化称编码（或序列化等），相反的过程称为解码（或解析，反序列化）

语言内置的编码库存在问题，包括和语言绑定、兼容性、效率

**JSON、XML、CSV&#x20;**&#x4F5C;为数据交换格式收到欢迎，尽管存在缺点，包括：

* 数字编码的模糊，XML 和 CSV 无法区分数字和数字组成的字符串，JSON 不区分整数和浮点数

* JSON 和 XML 不支持字节序列

* JSON 和 XML 支持 schema，但 CSV 不支持

**二进制编码**

编码大小可能更小，解析可能更快

![](data-intensive/image-12.png)

MessagePack

![](data-intensive/image-13.png)

**Thrift 和 ProtocolBuffers**

需要 schema 来编码，使用代码生成工具生成支持多种语言的类，可以调用生成的代码来编码或解码 schema

Thrift 使用 idl 描述模式，ProtocolBuffers 类似

Thrift 有 2 种二进制编码格式，分别为 BinaryProtocol 和 CompactProtocol

![](data-intensive/image-2.png)

![](data-intensive/image-5.png)

ProtocolBuffers 有一种

![](data-intensive/image-14.png)

required 和 optional 不记录在编码中，区别于，required 字段如果未填充运行检查出错，可用于捕获错误

模式演化同时保持向后和向前兼容

不能更改标签（数字 1，2，3）（导致现有编码无效），可更改字段名称，可添加新字段（旧读新直接忽略，新读旧新添字段只能为 optional），可删除字段（类似添加字段，只能删除 optional 字段，不能再使用相同标签号）

数据类型演化，32 位变为 64 位可以向前兼容，但无法向后兼容

ProtocolBuffers 无列表或数组类型，repeated 标记表示一个字段重复多次出现，可以将 optional（单值）转变为 repeated，新读旧看到包含 1 个或 0 个元素列表，旧读新只看到列表最后一个元素

Thrift 有专用列表数据类型，不支持单值到多值，但有支持嵌套列表优点

**Avro**

用于适配 Hadoop

两种模式语言，AvroIDL 和基于 JSON

![](data-intensive/image-1.png)

![](data-intensive/image.png)

编码无类型信息，按照出现的顺序遍历解析字段，所以模式必须完全匹配

写模式和读模式不必完全一样，保持兼容即可，数据被读取时，Avro 库通过对比查看写模式和读模式并将数据从写模式转换为读模式来解决其差异

![例如，如果写模式和读模式的字段顺序不同，这也没有问题，因为模式解析通过字段名匹配字段。如果读取数据的代码遇到出现在写模式但不在读模式中的字段，则忽略它。如果读取数据的代码需要某个字段，但是写模式不包含该名称的字段，则使用在读模式中声明的默认值填充。](data-intensive/image-28.png)

使用 Avro，向前兼容意味着可以将新版本的模式作为 writer，并将旧版本的模式作为 reader。相反，向后兼容意味着可以用新版本的模式作为 reader，并用旧版本的模式作为 write

reader 如何确定特定数据采用哪个 writer？取决于上下文

*很多记录大文件*

avro 常见用途，存储大量记录大文件，所有记录采用相同的模式，writer 可以在文件开头包含 writer 的模式信息

*单独记录写入数据库*

不同记录模式会不同，每个编码记录开始包含一个版本号，在数据库中保留一个模式版本列表，reader 根据记录的版本号查询对应的 writer 模式（Espresso）

*网路连接发送记录*

两个进程通过双向网络通信，可以在建立连接是协商模式版本，连接的生命周期中使用该模式（AvroRPC 协议原理）

Avro 本质是动态生成模式，模式中不包含标签号，字段通过名字标识，比如记录数据库每个表的模式，如果发生变化，可以直接动态生成新的 Avro 模式，更新的 writer 可以适配旧的 reader，而 Thrift 和 ProtocolBuffers 可能必须手动分配字段标签，需要维护列名到字段标签的映射

Thrift 和 ProtocolBuffers 依赖代码生成特性，适合静态语言，可以类型检查

Avro 可选代码生成，也可以不生成代码，在对象容器文件中包含元数据，使用 Avro 库打开它

基于模式的二进制编码的优点

* 比 JSON 更紧凑，可省略字段名称

* 模式是一种有价值的文档形式，必须保持最新，手动维护的文档可能过期

* 模式数据库允许在部署任何内容之前检查模式更改的向前和向后兼容性

* 对于静态类型编程语言的用户来说，从模式生成代码的能力是有用的，它能够在编译时进行类型检查

### 数据流模式

进程间（非共享内存）数据流

**通过网络或写入文件，将数据编码为字节序列**，一个编码，一个解码，独立升级不同部分，兼容性

**通过数据库**

可能只有一个进程访问，必须向后兼容

可能较新版本写入，较旧版本读，需要向前兼容

![](data-intensive/image-23.png)

数据重写为新模式，但代价较高，可以添加默认值为空而不重写，读取旧行为编码数据缺失的列填充为空值

底层存储可能包含多个版本模式，但整个数据库看起来像采用单个模式编码

归档存储，通常用最新模式编码

**通过服务调用**

REST 和 RPC

客户端（web 浏览器），javascript 客户端（Ajax）->web 服务器

SOA，微服务

两种流行 Web 服务：REST 和 SOAP

REST：基于 HTTP 原则，强调简单的数据格式，URL 表示资源，HTTP 功能缓存控制、身份验证和内容类型协商

SOAP：基于 XML 协议，API 请求，独立于 HTTP，避免使用大多数 HTTP 功能，有复杂的多种标准和新增功能，用 WSDL（WebServicesDescriptionLanguage，一种基于 XML 的语言）描述 API，依赖工具支持、代码生成和 IDE

RPC：远程过程调用，远程网络服务发出请求看起来与在同一进程中调用编程语言中的函数或方法相同

存在缺陷，网络请求和本地函数调用的不同：

* 本地函数调用是可预测的，并且成功或失败仅取决于控制的参数。网络请求不可预测（网络问题，远程服务器不可用……）

* 本地函数调用要么返回一个结果，要么抛出一个异常，或者永远不会返回（循环或进程崩溃）。网络请求另一个结果（一直收不到响应）

* 重试失败可能是请求已完成，但是响应丢失，重试会执行多次，幂等性问题

* 本地调用时间大致相同，网络请求可能因为网络阻塞或远程服务过载导致不同的时间

* 本地函数可以传递指针，网络请求需要编码成网络可以发送的字节序列，需要考虑较大的对象

* 网络请求有客户端和服务端语言不同的情况

二进制编码的自定义 RPC 协议，相比诸如 REST 的 JSON 类通用协议性能更好

RESTfulAPI 优点，有利于实验和调试，支持所有主流语言和平台，有庞大的工具生态系统。因此 REST 是公共 API 的主流风格，RPC 框架侧重于同一组织内不同服务之间的请求

向后向前兼容性取决于它所使用的具体编码技术，Thrift、gRPC 的编码格式，SOAP 的 XML 模式请求和响应，RESTfulAPIJSON 响应和 JSON 或 URI 编码的请求参数

**通过异步消息传递**

以低延迟传递到另一个进程（RPC 类似），不通过直接的网络连接发送消息，通过称为消息队列的中介发送

和 RPC 相比优点

* 接收方不可用中介可以充当缓冲区，提高可靠性

* 可以自动将消息重新发送到崩溃的进程，防止消息丢失

* 避免发送方需要知道接收方的 IP 地址和端口号

* 支持一条消息发送给多个接收方

* 逻辑上发送方和接收方分离

消息传递单向，发送方不期望收到回复

分布式 actor 框架

Actor 模型是用于单个进程中并发的编程模型。逻辑被封装在 Actor 中，而不是直接处理线程（以及竞争条件、锁定和死锁的相关问题）。每个 Actor 通常代表一个客户端或实体，它可能具有某些本地状态（不与其他任何 Actor 共享），并且它通过发送和接收异步消息与其他 Actor 通信。不保证消息传送：在某些错误情况下，消息将丢失。由于每个 Actor 一次只处理一条消息，因此不需要担心线程，每个 Actor 都可以由框架独立调度。

在分布式 Actor 框架中，这个编程模型被用来跨越多个节点来扩展应用程序。无论发送方和接收方是在同一个节点上还是在不同的节点上，都使用相同的消息传递机制。如果它们位于不同的节点上，则消息被透明地编码成字节序列，通过网络发送，并在另一端被解码



# 分布式数据系统

分布式目的

* 扩展性，单机器处理能力有限

* 容错与高可用性

* 延迟考虑，靠近客户端



**系统扩展**

垂直扩展和水平扩展

* 共享内存架构

由一个操作系统管理更多的 CPU，内存和磁盘，通过高速内部总线使每个 CPU 都可以访问所有的存储器或磁盘，容错有限，无法提供异地容错能力

成本增长过快甚至超过了线性：即如果把一台机器内的 CPU 数量增加一倍，内存扩容一倍，磁盘容量加大一倍，则最终总成本增加不止一倍。并且由于性能瓶颈因素，这样一台机器尽管拥有了两倍的硬件指标但却不一定能处理两倍的负载。

* 共享磁盘架构

拥有多台服务器，每个服务器各自拥有独立的 CPU 和内存，然后将数据存储在可共享访问的磁盘阵列上，服务器与磁盘阵列之间往往通过高速网络连接。

这种架构多适用于数据仓库等负载，然而通常由于资源竞争以及锁的开销等限制了其进一步的扩展能力

* 无共享架构（水平扩展）

运行数据库软件的机器或者虚拟机称节点。每个节点独立使用本地的 CPU，内存和磁盘。节点之间的所有协调通信等任务全部运行在传统网络（以太网）之上且核心逻辑主要依靠软件来实现。

无共享系统不需要专门的硬件，具有较高的性价比。它可以跨多个地理区域分发数据，从而减少用户的访问延迟，甚至当整个数据中心发生灾难时仍能继续工作。通过云计算虚拟机的部署方式，即便是小公司，也可以轻松拥有跨区域的分布式架构和服务能力。



数据分布面临的主要问题

**复制与分区**

将数据分布在多节点时有两种常见的方式：

*复制*

在多个节点上保存相同数据的副本，每个副本具体的存储位置可能不尽相同。复制方法可以提供冗余：如果某些节点发生不可用，则可以通过其他节点继续提供数据访问服务。复制也可以帮助提高系统性能

*分区*

将一个大块头的数据库拆分成多个较小的子集即分区，不同的分区分配给不同的节点（也称为分片）。我们在第 6 章主要介绍分区技术。

## 五、数据复制

目的

* 使数据在地理位置上更接近用户，从而降低访问延迟。

* 当部分组件出现故障，系统依然可以继续工作，从而提高可用性。

* 扩展至多台机器以同时提供数据访问服务，从而提高读吞吐量。

假设单台机器可以保存数据集完整副本

问题：数据持续更改

三种流行的复制数据变化的方法：主从复制、多主节点复制和无主节点复制

### **主节点与从节点**

1. 指定某一个副本为主副本（或称为主节点）。当客户写数据库时，必须将写请求首先发送给主副本，主副本首先将新数据写入本地存储。

2. 其他副本则全部称为从副本（或称为从节点）。主副本把新数据写入本地存储后，然后将数据更改作为复制的日志或更改流发送给所有从副本。每个从副本获得更改日志之后将其应用到本地，且严格保持与主副本相同的写入顺序。

3. 客户端从数据库中读数据时，可以在主副本或者从副本上执行查询。再次强调，只有主副本才可以接受写请求；从客户端的角度来看，从副本都是只读的。

应用：数据库，消息队列

**同步复制与异步复制**

![](data-intensive/image-18.png)

同步复制

优点：保证从节点读的时候数据最新

缺点：同步从节点无法确认影响写入成功

一般不会全部设置为同步复制，往往一个节点同步，其它从节点异步

**节点失效**

从节点失效：追赶恢复

从节点可以知道发生故障前处理的最后一笔事务，重新连接到主节点后，可以请求自那笔事务之后中断期间所有数据变更

主节点失效：节点切换

选择某个从节点提升为主节点，客户端之后的写请求发送到新的主节点，其它从节点接受新主节点上的变更数据

手动切换或自动切换

* 确认主节点失效

* 选举新的主节点

* 重新配置系统使新主节点生效，原先主节点恢复后降级

面临问题

* 异步复制新的主节点未收到原主节点数据就发生切换

* 两个节点同时自认为是主节点（脑裂）

* 设置合适的超时检测主节点失效



**复制日志实现**

1. 基于语句的复制

执行的每个写请求（操作语句）作为日志

不适用场景

* 调用非确定性函数语句，如 NOW（）

* 使用自增列或依赖于现有数据，副本需按照相同的顺序执行，当多个同时并发执行事务，有很大限制

* 有副作用的语句（例如，触发器、存储过程、用户定义的函数等），可能会在每个副本上产生不同的副作用

- 基于预写日志（WAL）传输

追加写日志

无论是日志结构存储引擎（SSTables 和 LSM-trees），还是修改预先写日志的 Btree，对数据库写入的字节序列都被介入日志

缺点：日志描述底层数据，包含了哪些磁盘块哪些字节改变，复制方案和存储引擎紧密耦合，如果数据库存储格式版本改变，系统通常无法支持主从节点软件版本不同

* 基于行的逻辑日志复制

复制和存储引擎采用不同的日志格式，通常是指一系列记录来描述数据表行级别的写请求

* 对于行插入，日志包含所有相关列的新值。

* 对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值。

* 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少包含所有已更新列的新值）。

Mysqlbinlog

逻辑日志和存储引擎逻辑解耦，可以容易地保持向后兼容

对外部应用程序（如用于离线分析的数据仓库），逻辑日志更容易解析

* 基于触发器的复制

注册自己的应用层代码，使得当数据库系统发生数据更改（写事务）时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑


### 复制滞后问题

主从复制延迟，具有最终一致性，但存在问题

**读自己的写**

用户写之后立马读，需要“写后读一致性”，能看到自己最近提交的更新，对其他用户没有保证

方案

* 访问可能被自己修改的内容，从主节点读取，否则在从节点读取，比如用户总是从主节点读自己的首页配置文件

* 跟踪最近更新的时间，如果更新后一分钟之内，在主节点读取，监控从节点复制滞后程度，避免从滞后时间超过 1 分钟从节点读取

* 客户端记住最近更新的时间戳，附带在读请求中，系统可以根据该信息确保提供的读至少包含了时间戳的更新

* 如果副本分布在多个数据中心，需路由到主节点所在数据中心

**单调读**

![](data-intensive/image-22.png)

单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况

实现单调读的一种方式是，确保每个用户总是从固定的同一副本执行读取（而不同的用户可以从不同的副本读取）。例如，基于用户 ID 的哈希的方法而不是随机选择副本

**前缀一致读**

对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写人的顺序

![](data-intensive/image-21.png)

如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常。然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值。

一个解决方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但该方案真实实现效率会大打折扣。现在有一些新的算法来显式地追踪事件因果关系，在本章稍后的“Happened-before 关系与并发”会继续该问题的探讨。

**解决方案**

应用层保证，例如只在主节点读取

事务保证

### 多主节点复制

配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似，处理写的每个主节点都必须将该数据更改转发到所有其他节点

适用场景

* 多数据中心，每个数据中心都配置主节点，性能更好（本地数据中心有主节点），容忍数据中心失效（数据中心故障可以不用切换主节点），容忍网络问题（多主一般用异步，容忍数据中心之间网络问题）

* 离线客户端操作，离线的更改会在下次设备联网时同步，每个设备都有一个充当主节点的本地数据库（接受写请求），联网后同步到服务端和其它设备

* 协作编辑，当一个用户编辑文档时，所做的更改会立即应用到本地副本（Web 浏览器或客户端应用程序），如果要确保不会发生编辑冲突，则应用程序必须先将文档锁定，然后才能对其进行编辑。锁来保证编辑不发生冲突。

**处理写冲突**

![](data-intensive/image-29.png)

**冲突检测**

多主节点两个写请求只能在之后异步时间点检测到冲突，如果用同步冲突检测，失去了每个主节点独立接受写请求优势

**冲突避免**

确保相同记录写请求总是通过同一个主节点

**收敛于一致状态**

更改复制同步之后，确保所有副本的最终值是相同的

方式

1. 每个写入分配唯一的 ID

2. 每个副本分配唯一的 ID

3. 将值合并，比如 5-7 中的 B/C

4. 预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突（可能会提示用户）

**自定义冲突解决逻辑**

应用层解决冲突

写入时执行：复制变更日志检测到冲突，调用应用层的冲突处理程序

读取时执行：将冲突写入值都暂存，下次读取数据，会将数据多个版本返回给应用层。应用层可能会提示用户或自动解决冲突，并将最后的结果返回到数据库

**拓扑结构**

复制的拓扑结构描述了写请求从一个节点的传播到其他节点的通信路径

![](data-intensive/image-24.png)

在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本。防止无限循环，每个节点赋予一个标识符，日志中每个写请求标记已通过节点标识符。如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，因此会忽略此变更请求，避免重复转发。

环形和星形拓扑的问题是，某个节点故障，可能会影响其他节点之间日志转发，可以重新配置拓扑结构排除故障节点。

全链接拓扑也存在一些自身的问题。主要是存在某些网络链路比其他链路更快的情况（例如由于不同网络拥塞），从而导致复制日志之间的覆盖，如图 5-9 所示。

![](data-intensive/image-16.png)

更新依赖于之前的插入。日志仅添加时间戳还不够，无法确保时钟完全同步，无法在主节点 2 上正确排序日志

为了使得日志消息正确有序，可以使用一种称为版本向量的技术

### 无主节点复制

去中心复制，所有副本直接接受写请求

某些无主节点系统实现客户端写请求直接发送到多副本，其它一些由一个协调者节点代表客户端写入，与主节点数据库不同，协调者不负责写入顺序维护

**节点失效**

无需主节点切换，失效节点恢复后同步，通过版本号区分新旧值

![](data-intensive/image-17.png)

**读修复与反熵**

读修复：并行读取多个副本，检测过期的返回值，发现过期写入新值

反熵：后台进程不断查找副本见数据差异，缺少的数据从一个副本复制到另一个副本，与主从复制日志不同，不保证以特定的顺序复制写人，并且会引入明显的同步滞后

**读写 quorum（限定数）**

r（读节点个数）>n（副本个数）-w（写入节点确认数）即可保证读取的节点中一定会包含最新值

常见选择：n 为奇数，w=r=（n+1）/2（向上舍入），也可灵活调整，如读多写少，设置 w=n 和 r=1 比较合适，这样读取速度更快，但是一个失效的节点就会使得数据库所有写人因无法完成 quorum 而失败

**Quorum 一致性的局限性**

通常设定 r 和 w 为简单多数，也可设置为较小的数字，w+r<=n，优点是等待节点数更少，并且允许更多副本无法访问情况依然能处理读取和写入

返回旧值情况

1. 写操作同时发生，需要处理写冲突

2. 写读同时发生，写仅在一部分副本完成

3. 总写入成功副本数小于 w，已成功副本不回滚，尽管写操作视作失败，后续读操作仍可能返回新值

……

**监控旧值**

监视是否返回最新结果，对于主从复制，维护复制日志执行的当前偏移量，对比主从节点当前偏移量的差值，衡量落后于主节点程度

对于无主节点复制系统，没有固定写入顺序，如果只支持读修复，旧值落后没有上限

**宽松的 quorum 与数据回传**

对于大规模集群（节点数>n）

宽松 quorum:只能连接到 n 个节点外的节点以满足 quorum，暂时写入到可访问的节点（不在 n 个节点中）

一旦网络问题得到解决，临时节点需要把接收到的写人全部发送到原始主节点上。这就是所谓的数据回传

提高写入可用性：只要有任何 w 个节点可用，数据库就可以接受新的写入。然而这意味着，即使满足 w+r>n，也不能保证在读取某个键时，一定能读到最新值，因为新值可能被临时写入 n 之外的某些节点且尚未回传过来

无法保证能从 r 个节点读到新值

**多数据中心**

无主节点复制也适用多数据中心

写入发送到所有副本，一般客户端等待本地数据中心 quorum 节点确认，远程数据中心通常配置为异步

**检测并发写**

![](data-intensive/image-25.png)

如何处理写冲突

**最后写入者获胜（丢弃并发写入）（lastwritewins，LWW）**

无法确定写请求"自然顺序"，强制对其排序，例如为每个写请求附加一个时间戳，选择最新即最大的时间戳，丢弃较早时间戳的写入

会牺牲数据持久性，同一个主键有多个并发写，最后只有一个写入值存活

**Happens-before 关系和并发**

如何判断两个操作并发

如果 B 知道 A，或者依赖于 A，或者以某种方式在 A 基础上构建，则称操作 A 在操作 B 之前发生。这是定义何为并发的关键。事实上，我们也可以简单地说，如果两个操作都不在另一个之前发生，那么操作是并发的

> 并发性、时间和相对性
>
> 通常如果两个操作“同时”发生，则称之为并发，然而事实上，操作是否在时间上重叠并不重要。由于分布式系统中复杂的时钟同步问题（第 8 章将会详细讨论），现实当中，我们很难严格确定它们是否同时发生。
>
> 为更好地定义并发性，我们并不依赖确切的发生时间，即不管物理的时机如何，如果两个操作并不需要意识到对方，我们即可声称它们是并发操作。一些人尝试把这个思路与物理学中狭义相对论联系起来，后者引入了“信息传递不能超越光速”的假定，如果两个事件发生的间隔短于光在它们之间的折返，那么这两个事件不可能有相互影响，因此就是并发。
>
> 在计算机系统中，即使光速快到允许一个操作影响到另一个操作，但两个操作仍可能被定义为并发。例如，发生了网络拥塞或中断，可能就会出现两个操作由于网络问题导致一个操作无法感知另一个，因此二者成为并发

**确定前后关系**

![](data-intensive/image-19.png)

* 服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号与写入的值一起保存。

* 当客户端读取主键时，服务器将返回所有（未被覆盖的）当前值以及最新的版本号。且要求写之前，客户必须先发送读请求。

* 客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合。写请求的响应可以像读操作一样，会返回所有当前值，这样就可以像购物车例子那样一步步链接起多个写入的值。

* 当服务器收到带有特定版本号的写入时，覆盖该版本号或更低版本的所有值（因为知道这些值已经被合并到新传入的值集合中），但必须保存更高版本号的所有值（因为这些值与当前的写操作属于并发）。

**合并并发值**

删除可能会出错，其中一个客户端删除，另一个没有，合并后出错

可以设计专门的数据结构自动执行合并，例如 Riak 的 CRDT 系列数据结构



## 六、数据分区

![](data-intensive/diagram-1.png)

分区：每一条数据（或者每条记录，每行或每个文档）只属于某个特定分区

目的：提高可扩展性，大数据集分散在更多的磁盘上，查询负载分布，每个节点可以独立执行查询

**数据分区与数据复制**

分区通常与复制结合使用，即每个分区在多个节点都存有副本。这意味着某条记录属于特定的分区，而同样的内容会保存在不同的节点上以提高系统的容错性。

一个节点上可能存储了多个分区。图 6-1 展示了主从复制模型与分区组合使用时数据的分布情况。由图可知，每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。一个节点可能即是某些分区的主副本，同时又是其他分区的从副本。

![](data-intensive/image-26.png)

### 键-值数据的分区

**基于关键字区间分区**

为每个分区分配一段连续的关键字或者关键字区间范围

每个分区内可以按照关键字排序保存，支持区间查询

**基于关键字哈希值分区**

避免数据倾斜，定义哈希函数

对于热点数据在关键字开头添加随机数，如 2 位随机数可以将写操作分布到 100 个关键字上，但读取也需要从所有 100 个关键字读取数据然后合并

### 分区与二级索引

二级索引不能唯一标识一条记录，只是用来加速特定值查询，挑战在于不能规整的映射到分区中

**基于文档分区的二级索引**

![](data-intensive/image-20.png)

每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档而不关心其他分区中数据

每当需要写数据库时，包括添加、删除或更新文档等，只需要处理包含目标文档 ID 的那一个分区

读取需要发送到所有分区，尽量单个分区满足二级索引

**基于词条的二级索引分区**

对所有的数据构建全局索引，而不是每个分区维护自己的本地索引，但索引也需要分区

![](data-intensive/image-27.png)

以待查找的关键字本身作为索引，可以通过关键词排序换分索引分区（如 a-r 在分区 0，s-z 在分区 1），也可以哈希划分

读取只用向包含词条的那一个分区发出读请求，写入会变慢，如果有多个二级索引并且分布在多个分区，导致写多个分区

一般不支持同步更新全局二级索引，采用异步更新

### 分区再平衡

* 查询压力增加，因此需要更多的 CPU 来处理负载。

* 数据规模增加，因此需要更多的磁盘和内存来存储数据。

* 节点可能出现故障，因此需要其他机器来接管失效的节点。

要求数据和请求可以从一个节点转移到另一个节点，这样一个迁移负载的过程称为再平衡

无论对于哪种分区方案，分区再平衡通常至少要满足：

* 平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。

* 再平衡执行过程中，数据库应该可以继续正常提供读写服务。

* 避免不必要的负载迁移，以加快动态再平衡，并尽量减少网络和磁盘 I/O 影响

**动态再平衡的策略**

为什么不取模？如果节点数变化，导致很多关键字节点迁移

**固定数量分区**

创建远超实际节点数的分区数，然后为每个节点分配多个分区

![](data-intensive/image-15.png)

分区的数量往往在数据库创建时就确定好，之后不会改变。原则上也可以拆分和合并分区（稍后介绍），但固定数量的分区使得相关操作非常简单，因此许多采用固定分区策略的数据库决定不支持分区拆分功能。所以，在初始化时，已经充分考虑将来扩容增长的需求（未来可能拥有的最大节点数），设置一个足够大的分区数。而每个分区也有些额外的管理开销，分区数量过多可能会有副作用

如果数据集的总规模高度不确定或可变，此时如何选择合适的分区数就有些困难。每个分区包含的数据量的上限是固定的，实际大小应该与集群中的数据总量成正比。如果分区里的数据量非常大，则每次再平衡和节点故障恢复的代价就很大；但是如果一个分区太小，分区数量就会太多，产生太多的开销。分区大小应该“恰到好处”，不要太大，也不能过小，如果分区数量固定了但总数据量却高度不确定，就难以达到一个最佳取舍点。

**动态分区**

分区大小超过阈值，拆分为两个分区，分区缩小到某个阈值以下，与相邻分区合并

与固定数量的分区一样，每个分区总是分配给一个节点，而每个节点可以承载多个分区。

当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载

**按节点比例分区**

分区数与集群节点数成正比关系，每个节点具有固定数量的分区

当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系；当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法也使每个分区大小保持稳定

新增节点，随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点，要求采用基于哈希分区

### 请求路由

![](data-intensive/image-35.png)

也可以依靠独立的协调服务（如 ZooKeeper）跟踪集群范围内的元数据

![](data-intensive/image-30.png)

**并行查询执行**

对于大规模并行处理（massivelyparallelprocessing，MPP）这一类主要用于数据分析的关系数据库，在查询类型方面要复杂得多。典型的数据仓库查询包含多个联合、过滤、分组和聚合操作。MPP 查询优化器会将复杂的查询分解成许多执行阶段和分区，以便在集群的不同节点上并行执行。尤其是涉及全表扫描这样的查询操作，可以通过并行执行获益颇多。



## 七、事务

![](data-intensive/diagram-2.png)

将应用程序的多个读、写操作捆绑在一起成为一个逻辑操作单元。即事务中的所有读写是一个执行的整体，整个事务要么成功（提交）、要么失败（中止或回滚）。如果失败，应用程序可以安全地重试。这样，由于不需要担心部分失败的情况（无论出于何种原因），应用层的错误处理就变得简单很多。

> 事务不是一个天然存在的东西，它是被人为创造出来，目的是简化应用层的编程模型。有了事务，应用程序可以不用考虑某些内部潜在的错误以及复杂的并发性问题，这些都可以交给数据库来负责处理（我们称之为安全性保证）。

### **深入理解事务**

事务与可扩展性并不完全对立，也不是必备功能

**ACID 含义**

原子性（Atomicity），一致性（Consistency），隔离性（Isolation）与持久性（Durability）

与 ACID 对立，BASE，基本可用性（BasicallyAvailable），软状态（Softstate）和最终一致性（Eventualconsistency）

**原子性**

在出错时中止事务，并将部分完成的写人全部丢弃。如果事务已经中止，应用程序可以确定没有实质发生任何更改，所以可以安全地重试

**一致性**

对数据有特定的预期状态，任何数据更改必须满足这些状态约束（或者恒等条件）。例如，对于一个账单系统，账户的贷款余额应和借款余额保持平衡。如果某事务从一个有效的状态开始，并且事务中任何更新操作都没有违背约束，那么最后的结果依然符合有效状态。

本质上要求应用层来维护状态一致，应用程序有责任正确地定义事务来保持一致性

**隔离性**

多个客户端访问相同记录的并发问题

并发执行的多个事务相互隔离，它们不能互相交叉

**持久性**

保证一旦事务提交成功，即使存在硬件故障或数据库崩溃，事务所写入的任何数据也不会消失

**单对象与多对象事务操作**

原子性和隔离性主要针对客户端在同一事务中包含多个写操作时，数据库所提供的保证。单对象也同样适用。

**多对象事务的必要性**

关系数据模型，某一行是另一个外键。

文档数据模型，可能需要一次更新多个文档

二级索引数据库，需要同步更新索引

**处理错误与中止**

重试中止的事务虽然是一个简单有效的错误处理机制，但它并不完美

### 弱隔离级别

可串行化性能较低，为了不牺牲性能，更多倾向于采用较弱的隔离级别，它可以防止某些但并非全部的并发问题

#### **读-提交**

读数据库只能看到已成功提交的数据（防止脏读）

写数据库时只会覆盖已成功提交的数据（防止脏写）

防止看到部分更新，防止写操作回滚时看到被回滚的数据（实际这些数据不会提交）

对于是尚未提交事务的先前的写入，防止另一个事务提交覆盖先前的写入。通常通过推迟第二个写请求，直到前面的的事务提交

![](data-intensive/image-34.png)

**实现读-提交**

通常采用行级锁来防止脏写

防止脏读，一种选择是使用相同的锁，所有试图读取该对象的事务必须先申请锁，事务完成后释放锁。但运行时间较长的写事务会导致许多只读的事务等待太长时间，这会严重影响只读事务的响应延迟

大多数据库方法：对于每个待更新的对象，数据库都会维护其旧值和当前持锁事务将要设置的新值两个版本。在事务提交之前，所有其他读操作都读取旧值；仅当写事务提交之后，才会切换到读取新值

#### **快照级别隔离与可重复读**

![读倾斜，对于读提交的问题，不可重复读](data-intensive/image-33.png)

快照级别隔离，解决常见手段。其总体想法是，每个事务都从数据库的一致性快照中读取，事务一开始所看到是最近提交的数据，即使数据随后可能被另一个事务更改，但保证每个事务都只看到该特定时间点的旧数据。

**实现快照级别隔离**

写加锁防止脏写，但读取不加锁

实现：保留了对象多个不同的提交版本，这种技术因此也被称为多版本并发控制（Multi-VersionConcurrencyControl，MVCC）

PostgreSQL 实现方式（其它类似）：每行额外两个字段，created\_by 字段表示创建该行事务 ID，deleted\_by 字段表示删除该行事务 ID，更新内部转换为一个删除操作和一个创建操作

**快照可见性规则**

1. 事务开始时列出当前正在进行的事务，之后忽略这些事务的写入

2. 中止事务不可见

3. 较晚事务不可见

4. 除此之外的事务写入可见

**索引和快照级别隔离**

一种方案：索引直接指向对象的所有版本，然后想办法过滤对当前事务不可见的那些版本

另一种方案：追加式 B-tree，每个写入事务创建新的 B-treeroot，修改时创建新的修改副本，拷贝必要的内容，让上面的节点指向新创建的节点

#### **防止更新丢失**

对于 read-modify-write 过程，两个事务在同样的数据对象执行类似操作，由于隔离性，第二个写操作不包含第一个事务修改后的值，导致第一个事务修改至可能会丢失

解决方案：

**原子写操作**

数据库层

UPDATEcountersSETvalue=value+1WHEREkey='foo'并发安全

**显式加锁**

数据库不支持内置原子操作，应用程序显示锁定待更新对象，如 select...forupdate

**自动检测更新丢失**

原子操作和锁本质是让“读-修改-写回”的序列串行执行，另一种思路是先让他们并发执行，如果事务管理器检测到了更新丢失风险，则会中断当前事务，强制回退到安全的“读-修改-写回”方式

借助快照级别隔离可以高效执行检查，但不是所有数据库支持检测更新丢失

**原子比较和设置**

只有在上次读取的数据没有发生变化时才允许更新；如果已经发生了变化，则回退到“读-修改-写回”方式。

**冲突解决与复制**

多副本数据库，不同节点可能会并发修改数据，需采取额外措施防止丢失更新

#### **写倾斜与幻读**

设想这样一个例子：你正在开发一个应用程序来帮助医生管理医院的轮班。通常，医院会安排多个医生值班，医生也可以申请调整班次（例如他们自己生病了），但前提是确保至少一位医生还在该班次中值班。

现在情况是，Alice 和 Bob 是两位值班医生。两人碰巧都感到身体不适，因而都决定请假。不幸的是，他们几乎同一时刻点击了调班按钮。

每笔事务总是首先检查是否至少有两名医生目前在值班。如果是的话，则有一名医生可以安全里离开。由于数据库正在使用快照级别隔离，两个检查都返回有两名医生，所以两个事务都安全地进入到下一个阶段。接下来 Alice 更新自己的值班记录为离开，同样，Bob 也更新自己的记录。两个事务都成功提交，最后的结果却是没有任何医生在值班，显然这违背了至少一名医生值班的业务要求。

**定义写倾斜**

广义的更新丢失。如果两个事务读取相同的一组对象，更新其中一部分，不同的事务可能更新不同的对象，可能发生写倾斜，不同的事务如果更新同一个对象，可能发生脏写或更新丢失

**更多写倾斜的例子**

会议室预定系统，多人游戏，声明一个用户名，防止双重开支

**为何产生写倾斜**

写倾斜的规律

1. select 查询满足条件的行（例如，至少有两名医生正在值班，同一时刻房间没有预订，棋盘的某位置没有出现数字，用户名还没有被占用，账户里还有余额等）

2. 根据查询的结果，应用层决定是否继续操作

3. 如果继续，发起写入

写入会改变步骤 2 做出决定的前提条件，即提交写入后重复执行步骤一的 select 查询，会返回完全不同的结果

这种在一个事务中的写入改变了另一个事务查询结果的现象，称为幻读

### 串行化

* 隔离级别难以理解，不同数据库实现不一致

* 检查应用层代码，很难判断它在特定隔离级别下是否安全

* 缺乏工具检测竞争状况

解决方案：串行化，即使事务可能会并行执行，最终结果与每次一个即串行执行结果相同，防止所有竞争条件

#### 实际串行执行

在一个线程上按顺序方式每次只执行一个事务

**采用存储过程封装事务**

![](data-intensive/image-37.png)

**分区**

串行执行所有事务使得并发控制更加简单，但是数据库的吞吐量被限制在单机单个 CPU 核。虽然只读事务可以在单独的快照上执行，但是对于那些高写入需求的应用程序，单线程事务处理很容易成为严重的瓶颈。

对数据分区，每个事务只在单个分区内读写数据，每个分区都可以有自己的事务处理线程且独立运行

跨分区事务需要额外的协调，性能要低很多

**串行执行小结**

* 事务必须简短而高效，否则一个缓慢的事务会影响到所有其他事务的执行性能。

* 仅限于活动数据集完全可以加载到内存的场景。有些很少访问的数据可能会被移到磁盘，但万一单线程事务需要访问它，就会严重拖累性能。

* 写入吞吐量必须足够低，才能在单个 CPU 核上处理；否则就需要采用分区，最好没有跨分区事务。

* 跨分区事务虽然也可以支持，但是占比必须很小

#### 两阶段加锁

two-phaselocking,2PL

读写锁

第一阶段事务执行前获取锁，第二阶段事务结束释放锁

死锁，数据库系统自动检测

缺点：性能下降，访问延迟不确定性，可能因为锁等待很长时间

**谓词锁**

防止写倾斜和幻读（如会议室预定场景）

谓词锁：不属于某个特定对象，作用于满足某些搜索条件的所有查询对象

谓词锁会限制如下访问：

* 如果事务 A 想要读取某些满足匹配条件的对象，例如采用 SELECT 查询，它必须以共享模式获得查询条件的谓词锁。如果另一个事务 B 正持有任何一个匹配对象的互斥锁，那么 A 必须等到 B 释放锁之后才能继续执行查询

* 如果事务 A 想要插入、更新或删除任何对象，则必须首先检查所有旧值和新值是否与现有的任何谓词锁匹配（即冲突）。如果事务 B 持有这样的谓词锁，那么 A 必须等到 B 完成提交（或中止）后才能继续

**索引区间锁**

谓词锁性能差，大部分 2PL 使用索引区间锁（next-keylocking），本质是对谓词锁的简化或近似



#### 可串⾏化的快照隔离

提供了完整的可串⾏性保证，⽽性能相⽐于快照隔离损失很⼩

基于快照隔离，读取基于快照，同时增加了相关算法检测写入冲突来决定是否中止事务

事务基于某些前提条件成立而执行，如果提交时条件发生变化，则需中止

数据库需检测事务是否会修改其他事物的查询结果，具体分以下 2 种情况

1. 是否读取的是一个过期的 MVCC 对象

事务读取时是其它某个事务未提交的对象，提交时该对象已经被其它某个事务提交，

* 写入是否影响即将完成的读取

![](<data-intensive/截屏2025-06-24 17.43.41.png>)

事务 42 的写影响了事务 43 的读取（42 先于 43 提交）



## 八、分布式系统挑战

![](data-intensive/diagram-3.png)

与单节点系统差异显著，会出现各种问题

核心：构建可靠系统

认清分布式系统状态本质->评估所发生的各种故障

### 故障与部分失效

单节点运行具有确定性，当硬件正常时，主要问题是 bug，且相同的操作通常会产生相同的结果

分布式系统会出现部分失效，即一部分工作正常，一部分出现故障，难点在于部分失效的不确定：如有时网络正常，有时失败

#### 云计算和超算

构建大规模计算系统思路

1. 高性能计算，包含成千上万个 CPU 的超级计算机构成一个庞大的集群，通常用于计算密集型科学计算

2. 云计算

不同构建方式处理错误方法也不同。高性能计算通常会对任务保存快照，出现故障停止整个集群，修复后从最近快照执行（更像是一个单节点系统）

本书重点是基于互联网的服务，与 HPC 不同

1. 大部分是在线服务

2. HPC 通常采用专用的硬件，每个节点可靠性高，节点间主要通过共享内存或远程内存直接访问等技术通信。云计算节点多由通用机器构建，单节点成本低廉

3. 大型数据中心通常基于 IP 和以太网，HPC 通常基于特定的网络拓扑结构

4. 可容忍失败节点，让整体继续工作

5. 数据中心分布全球部署（让用户访问地理靠近的数据中心，降低延迟），HPC 通常假设节点位置靠近

必然面临部分失效，依靠软件系统提供容错机制，即在不可靠的组件上构建可靠的系统

### 不可靠的网络

主要关注分布式无共享系统：通过网络连接的多个节点，网络是跨节点通信唯一路径

互联网大多数据中心内部网络基于异步网络，节点通过网络发送消息到另一个节点，但网络不保证什么时候到达及是否到达。发送到等待响应过程中，可能出错的事情

1. 请求已丢失

2. 请求在某个队列中等待，无法马上发送

3. 远程接收节点已失效（如崩溃或关机）

4. 远程接收节点暂时无法响应

5. 远程接收节点已经完成请求处理，但回复在网络中丢失

6. 远程接收节点已经完成请求处理，回复被延迟处理

![](<data-intensive/截屏2025-06-25 15.51.37.png>)

#### 检测故障

需要自动检测节点失效

#### 超时和无限的延迟

延迟理论无限大，大多数系统无法保障数据包传输的延迟和处理数据包的时间

超时时间不好设置

**网络拥塞和排队**

延迟变化根源往往在于排队

1. 网络交换机处理大量数据包时的排队

2. 目标机器 cpu 均繁忙时，会被操作系统处理排队

3. 虚拟化环境 cpu 切换虚拟机导致的排队

4. tcp 流量控制的排队，以及 tcp 重传引入的延迟

### 不可靠的时钟

许多场景依赖时钟，包括判断请求超时，服务 p99，用户浏览时间，文章发表时间，缓存过期时间……

NTP（Network Time Protocol）网络时间协议：可以根据一组专门的时间服务器调整本地时间



## 九、一致性与共识

![](data-intensive/diagram-4.png)

类比事务的抽象可以让应用程序可以假装没有崩溃（原⼦性），没有与其他⼈并发访问数据库（隔离性），且存储设备是完全可靠的（持久性），尝试建立可以让分布式系统应用忽略内部各种问题的抽象机制

**重要抽象之一：共识**

分布式系统可提供的若干保证和抽象机制  -->  解决共识问题的相关算法



### 一致性保证

最终一致性：停止更新数据库，等待一段时间后最终所有读请求会返回相同的内容。即不一致现象是暂时的，最终会达到一致

更强的一致性模型，意味着更多代价，如性能降低或容错性降低



### 可线性化

![非线性化例子](<data-intensive/截屏2025-06-27 14.35.10.png>)

可线性化基本思想：**使系统看起来好像只有一个数据副本**

![一个重要约束](<data-intensive/截屏2025-06-27 14.56.19.png>)

![](<data-intensive/截屏2025-06-27 15.01.41.png>)

CAP 理论，在出现网络故障时，一致性（线性化）和可用性无法兼具

线性化和性能之间的取舍



### 顺序保证

线性化的操作按某种顺序执行（图 9-4）

一些顺序要求的场景：

* 主从复制时，主节点确定复制日志写入顺序

* 可串行化确保的事务执行结果与按照某种顺序方式执行一样。

#### 顺序与因果关系

顺序的作用：保障因果关系

要求因果关系的一些场景：

* 图 5-5，先有问题的答案，再有问题的本身，违背了因果关系

* 图 5-9，需要先有创建数据行，后有更新

* 第 5 章“检测并发写”，两个操作 A 和 B，三种关系，A 发生在 B 之前，B 发生在 A 之前，A 和 B 并发，前两者都是因果关系

* 事务快照隔离，事务从一致性快照读取，一致性即与因果关系一直，如果快照包含了答案，那么它也应该包含所提的问题

* 事务写倾斜，图 7-8 调班动作的因果关系取决于当前值班人，可序列化的快照隔离（可串行化的快照隔离）主要通过跟踪事务之间的因果依赖关系来达到检测写倾斜目的

**因果顺序并⾮全序**

全序：支持任意两个元素的比较

因果：某两个事件的关系，一个发生在另一个前

#### 序列号排序

使用序列号排序事件

存在唯一主节点，可以让主节点为每个操作递增某个计数器，从而为复制日志中的每个操作赋值一个单增的序列号

**非因果序列发生器**

不存在唯一主节点，产生序列号较复杂

三个思路

1. 每个节点独立产生自己的一组序列号，比如 A 产生奇数，B 产生偶数，或者 A 产生 1-1000，B 产生 1000-2000

2. 时间戳信息（物理时钟）附加到操作上

问题：产生的序列号与因果关系不严格一致

**Lamport 时间戳**

值对（计数器，节点 ID）

![](data-intensive/image-36.png)

最大计数器值嵌入到每一个请求中，如果发现收到的响应或请求中的最大值大于自身的技术器值，则把自己的计数器修改为该最大值

#### 全序关系广播

比喻：想象一个聊天群，里面有多个成员。全序关系广播要保证：无论消息从哪个成员发出，也无论每个成员在什么时间收到消息，所有成员看到的聊天记录顺序必须完全一致。

全序关系广播是一种消息传递协议，它确保在分布式系统的所有节点上，消息的传递顺序是完全一致的。它需要满足以下**两个核心属性**：

1. 可靠广播：如果一个正确的（非故障的）节点广播了一条消息，那么所有正确的节点最终都会收到这条消息。（不会丢失消息）

2. 全序传递：任何两个正确的节点，它们收到所有消息的顺序都是完全相同的。

**为什么需要它？**

在分布式系统（如数据库、状态机）中，多个副本需要保持完全一致的状态。实现这一目标最有效的方法就是让所有副本都以相同的顺序执行相同的操作指令。全序广播正是用来保证所有副本收到操作指令的顺序一致的机制。

实现技术：

常见的共识算法，如 Paxos 和 Raft，其核心功能之一就是实现全序关系广播。在 Raft 中，领导者（Leader）负责为所有客户端请求（即日志条目）确定一个唯一的顺序，并将这个顺序复制到所有追随者（Follower）节点上。



全序关系广播是实现线性化存储的一种强大工具。

* 如果你有一个全序关系广播系统，你可以很容易地构建一个线性化的存储。

  * 方法：将所有写操作（有时也包括读操作）通过全序广播发送给所有副本。每个副本按照广播确定的全局唯一顺序来依次执行这些操作。由于所有副本以相同顺序执行相同操作，它们的最终状态会保持一致，并且整个系统对外表现为线性化。



### 分布式事务与共识

需要达成一致的场景

* 主节点选举

* 原子事务提交

#### 原子提交和2PC（两阶段提交）

![](data-intensive/image-32.png)

![](data-intensive/image-31.png)

#### 实践中的分布式事务

##### **Exactly-once 消息处理**

* 在分布式系统中，消息可能因为网络抖动或服务故障而被**重复发送**。

* 如果处理逻辑不是幂等的（idempotent），重复处理会导致数据错误。

* Exactly-once 的目标是：**无论消息被发送多少次，处理结果都只发生一次且正确。**

**⚙️ 技术策略：如何实现 Exactly-once？**

1. **使用唯一消息 ID（Message ID）**

   * 每条消息附带一个唯一标识符。

   * 消费者记录已处理的 ID，避免重复处理。

2. **幂等操作（Idempotent Operations）**

   * 设计处理逻辑，使得重复执行不会改变最终结果。

   * 例如：数据库插入使用 UPSERT、更新使用 SET 而非 INCREMENT。

3. **事务性处理（Transactional Processing）**

   * 将消息消费与状态更新放入同一个事务中。

   * 如果事务失败，消息不会被标记为已处理。

4. **去重存储（Deduplication Store）**

   * 使用 Redis、数据库或日志系统记录已处理消息 ID。

   * 注意存储的 TTL 和性能影响。

**📌 实践挑战与权衡**

* **性能 vs. 精度**：Exactly-once 通常比 At-least-once 慢，因为需要额外的存储和检查。

* **状态一致性**：消费者必须维护处理状态，可能引入复杂的状态同步问题。

* **系统设计**：Kafka、RabbitMQ 等消息系统本身不保证 Exactly-once，需要应用层配合。

##### **XA交易**

**XA（eXtended Architecture）** 是由 X/Open 标准制定的分布式事务处理协议。

它用于在多个数据库或消息系统之间协调事务，确保满足 **ACID**（原子性、一致性、隔离性、持久性）属性。

支持 XA 的系统和工具

* 常见数据库：PostgreSQL、MySQL、DB2、SQL Server、Oracle 等。

* 消息中间件：ActiveMQ、HornetQ、MSMQ、IBM MQ 等

XA 使用 **两阶段提交协议（2PC）** 来确保所有参与者一致提交或回滚事务。

事务管理器负责协调各个资源的准备和提交过程。



##### **协调者故障中恢复**

当系统组件发生故障时，恢复不仅仅是重启，而是要**恢复到一致的状态**。

为了实现这一目标，系统需要依赖**日志记录**和**协调机制**。

**📚 关键概念拆解**

1. **日志的重要性**：

   * 每个组件记录自己的操作日志。

   * 恢复时通过重放日志来恢复状态。

   * 但日志可能不完整或顺序不一致，需协调。

2. **一致性判断的挑战**：

   * 系统需要判断哪些操作已经完成，哪些未完成。

   * 如果没有协调机制，可能会出现状态不一致或数据丢失。

3. **协调机制的作用**：

   * 协调多个组件的日志与状态，确保恢复后系统整体一致。

   * 类似于分布式事务中的两阶段提交（2PC）或更复杂的共识协议（如 Paxos、Raft）。

**实践中的恢复策略**

* **检查点（Checkpoint）**：定期保存系统状态，减少恢复时的复杂度。

* **幂等性设计**：确保操作可以重复执行而不会造成副作用。

* **恢复流程的验证**：系统设计中必须考虑如何验证恢复的正确性。

##### 分布式事务限制

* **分布式事务（如 XA）虽然能提供强一致性，但代价高昂**。

* 在大规模系统中，XA 的性能瓶颈和复杂性使其难以落地。

* 许多现代系统（如 HDFS、HBase）选择放弃 XA，采用更灵活的方式处理一致性。

**XA 的问题**：

* 实现复杂，依赖底层资源管理器的支持。

* 性能开销大，尤其在高并发场景下。

* 容错能力弱，一旦协调者故障，可能导致事务阻塞。

**2PC 的局限性**：

* **阻塞问题**：参与者在等待协调者指令时无法继续处理其他事务。

* **单点故障**：协调者崩溃后，系统可能陷入不确定状态。

* **缺乏自动恢复机制**：无法应对网络分区或节点重启后的状态恢复。

**一致性 vs 可用性**：

* 在 CAP 理论下，强一致性往往牺牲了可用性。

* 某些系统选择最终一致性或幂等操作来提升可用性。



#### 支持容错的共识

* 在分布式系统中，多个节点需要就某个事实达成一致（consensus），即使存在网络延迟、节点故障或恶意行为。

* 共识协议的目标是确保系统在不可靠环境下仍能做出一致决策。

**📐 共识协议的基本性质**

1. **一致性（Uniform Agreement）**

   * 所有非故障节点最终必须达成相同的决策。

2. **有效性（Validity）**

   * 如果某个节点提出了一个值，最终决定的值必须是某个节点曾提出过的。

3. **终止性（Termination）**

   * 所有非故障节点最终都必须做出决定，不能无限等待。

##### 共识算法与全序广播

* **共识算法（Consensus Algorithm）**：用于在多个节点之间就某个值达成一致，确保系统状态一致性。

* **全序广播（Total Order Broadcast）**：确保所有节点以相同顺序接收并处理消息，是实现状态机复制的关键。

两者密切相关：实现全序广播的本质就是实现共识。

**⚙️ 核心协议对比：VSR、Paxos、Raft、Zab**

**一致性与共识的要点**

1. **节点可能崩溃，消息可能丢失** → 共识算法必须具备容错能力。

2. **容错的基础是消息不能丢失** → 需持久化日志、重试机制。

3. **VSR、Raft、Zab 的决策需稳定议员确认** → 强调领导者与多数派的角色。

4. **Paxos 的决策更灵活** → 只要满足 2M 的条件（多数派），任何节点可提议。

##### 主从复制与共识

在主从架构中，**所有写操作由主节点处理**，然后同步到从节点。

表面上看，主节点决定一切，似乎不需要共识。

但实际上，为了保证系统的**一致性与可靠性**，共识机制仍然不可或缺。

1. **主节点的决策是否可靠？**

* 如果主节点出现故障或行为异常（如脑裂），可能导致数据不一致。

* 因此，系统需要一种机制来验证主节点的决策是否被多数节点认可。

- **主节点的选举过程**

* 当主节点失效时，系统必须选出新的主节点。

* 这个过程本质上就是一个**共识问题**：多个节点必须就“谁是新主”达成一致。

* 常见协议如 Raft、Zab 都将主节点选举作为核心部分。

- **复制策略与一致性保障**

* 主节点在写入数据后，需将变更同步到从节点。

* 若同步机制不具备确认机制（如 ACK），可能导致数据丢失或不一致。

* 共识协议可以确保写入在多数节点上达成一致后才算成功。

##### Epoch和Quorum

1. Epoch（世代编号）

* **Epoch Number / Term Number / Ballot Number**：

  * 都是用于标识某一轮共识或领导者任期的编号。

  * 在 Raft 中称为 Term，在 Paxos 中称为 Ballot。

* **作用**

  * 将系统时间切分为不重叠的世代（epoch）

  * 每个 epoch 内 Leader 唯一且确定

  * 单调递增，保证新的 Leader 一定“更新”于旧 Leader

- Leader 选举与确认流程

  1. **故障检测**

     * 若怀疑当前 Leader 失效，发起新一轮选举

  2. **新 Leader 产生**

     * 投票产生一个更高的 epoch 编号

     * 如果出现两个 Leader → 编号高者胜

  3. **Leader 决策前检查**

     * 必须确认没有比自己更新的 epoch Leader 存在

     * 自身“认为是 Leader”并不够，需要 Quorum 认可

- Quorum（法定人数）要求

* **定义**：一个足够大的节点集合，能在决策上代表整个集群

* **常见情况**：多数节点（N/2 + 1），但并非必须是简单多数

* **作用**：

  * Leader 确认自己合法性

  * 对提案进行投票表决

* **关键约束**：选举 Quorum 与提案投票 Quorum 必须有 **重叠节点**，确保安全性（防止两个不同 epoch 的决议同时被提交）

- 两轮投票

  1. **轮 1**：选举 Leader（产生 epoch）

  2. **轮 2**：Leader 提案投票（带 epoch 号）

  * 如果提案投票中未出现更高的 epoch，说明 Leader 没被取代 → 提案安全可提交

##### 共识的局限性

**共识的优势与代价**

**✅ 优势（前提背景）**

* **安全属性**：一致性（Consistency）、完整性（Integrity）、有效性（Validity）

* **容错能力**：多数节点存活即可继续服务

* **线性化保证**：通过全序关系广播（Total Order Broadcast）实现原子操作的线性化语义

**&#x20;限制与成本**

#### 成员与协调服务

**类型**：分布式键值存储（Distributed KV Store）/ 协调与配置服务（Coordination & Configuration Service）

**主要用途**：为分布式系统提供可靠的小规模元数据存储与一致性保证

**非目标**：存储大量业务数据（其数据量可全部放入内存，磁盘仅用于持久化）

1. **为什么需要共识**

2. **🔑 核心特性**

   1. **线性化原子操作 (Linearizable Atomic Ops)**

      * 例如：CAS（Compare-And-Set）实现分布式锁

      * 结合租约（lease）机制，防止客户端失效后锁长期占用

   2. **操作全序 (Total Order)**

      * 所有操作赋予单调递增的事务 ID (zxid) + 版本号

      * 支持 fencing token 防止进程暂停导致的锁冲突

   3. **故障检测 (Failure Detection)**

      * 会话与心跳机制（Session + Heartbeat）

      * 会话失效 → 自动删除临时节点 (ephemeral nodes) → 释放锁资源

   4. **变更通知 (Watch/Notification)**

      * 客户端可订阅节点变化（成员加入、离开、数据变化）

      * 避免频繁轮询

3. &#x20;依赖场景（生态中的作用）

很多分布式系统（HBase, Hadoop YARN, Kafka, OpenStack Nova 等）依赖 ZK/etcd 去解决：

* 集群成员管理（Membership Management）

* Leader 选举

* 分布式锁/租约

* 配置分发

* 元数据一致性

##### 节点任务分配

1. 核心场景

**单主故障切换（Leader Failover）**

* 多个实例中始终保持一个主节点

* 主挂掉时由其他节点接管

* 常见于主从复制数据库、作业调度器、有状态服务

**分区资源分配（Partition Assignment）**

* 多个分区（DB 分片、消息流、文件块、Actor 等）分配给不同节点

* 新节点加入 → 迁移部分分区实现动态负载均衡

* 节点离开/失效 → 其他节点接管其分区

- ZooKeeper 提供的能力支撑

- 架构上的取舍

* **外包协调**：应用不需要自己实现共识、全序广播、故障检测 → 降低复杂度

* **数据变化频率低**：例如 “分区7的主节点在 10.1.1.23” 这种信息，分钟/小时级更新即可

* **不适合高频实时状态**：如果是每秒数千到百万级变化（例如指标流、事件流），应考虑 BookKeeper、Kafka 这种流式或日志存储

##### 服务发现

1. 核心场景：服务发现的角色

* **目标**：让客户端动态找到可用服务的地址（IP/端口等）

* **挑战**：云环境中节点常常会频繁上下线，IP 动态变化，启动前很难提前知道彼此的位置

* **典型实现**：

  1. 节点启动时向 ZooKeeper / etcd / Consul 注册自身网络信息

  2. 其他组件通过查询注册表获取当前可用的服务地址

- 共识在服务发现中的取舍

> 现实中，DNS 的设计哲学是**可用性优先**：即使网络中断时依然尽可能返回缓存结果。这种策略适合多数服务发现场景。

* 🛠 共识系统与只读副本

- 某些共识系统支持 **只读缓存副本**（Read-only Replica）

  * 异步接收由投票节点达成的决议日志

  * 自身不参与投票

  * 适合提供不需要强一致的读服务（如非关键的服务发现查询）

##### 成员服务

1. **🧩 成员服务的核心概念**

   * 用于确定“当前集群有效成员”的系统组件

   * 常回答两个问题：

     1. 哪些节点被视为集群的一部分

     2. 这些节点目前是否处于“活跃”状态

2. **关键特点**

   1. **与故障检测绑定的共识**

      * 单纯的网络心跳 ≠ 可靠故障判断（因网络延迟不可避免）

      * 通过共识让所有存活节点**一致认定**某个节点是否失效

      * 即便存在误判，保证全体决策一致性仍是首要目标 （避免不同节点眼中的“集群成员列表”不一致）

   2. **典型应用场景**

      * **Leader 选举**：如选择编号最小的节点为主

      * **集群 reconfiguration**：添加、移除节点需要全体一致

      * **任务分配 / 决策投票**：前提是所有参与方对“谁在场”有相同认知

   3. **共识失败条件**

      * 若各节点对“当前集群成员集”存在分歧 → 共识过程无法继续

      * 因为协议参与方集合不一致会破坏安全性保证（投票多数派定义失效）

3. **📌 和之前讨论内容的联系**

   * **与协调服务的关系**

     * ZooKeeper / etcd 提供了内置的成员服务（Membership）能力

     * 这层能力是 Leader 选举、分区分配、锁服务的前提条件

   * **与 Epoch / Quorum 的关系**

     * Epoch 机制默认节点集合固定，一旦动态调整就需要成员服务介入

     * Quorum 的定义依赖于“有效成员集”的共识结果
