---
title: 数据密集型应用系统设计（5-6章）
date: 2025-09-28 20:39:32
categories:
   - [Read-Note, 数据密集型应用系统设计]
---

# 分布式数据系统

分布式目的

* 扩展性，单机器处理能力有限

* 容错与高可用性

* 延迟考虑，靠近客户端



**系统扩展**

垂直扩展和水平扩展

* 共享内存架构

由一个操作系统管理更多的 CPU，内存和磁盘，通过高速内部总线使每个 CPU 都可以访问所有的存储器或磁盘，容错有限，无法提供异地容错能力

成本增长过快甚至超过了线性：即如果把一台机器内的 CPU 数量增加一倍，内存扩容一倍，磁盘容量加大一倍，则最终总成本增加不止一倍。并且由于性能瓶颈因素，这样一台机器尽管拥有了两倍的硬件指标但却不一定能处理两倍的负载。

* 共享磁盘架构

拥有多台服务器，每个服务器各自拥有独立的 CPU 和内存，然后将数据存储在可共享访问的磁盘阵列上，服务器与磁盘阵列之间往往通过高速网络连接。

这种架构多适用于数据仓库等负载，然而通常由于资源竞争以及锁的开销等限制了其进一步的扩展能力

* 无共享架构（水平扩展）

运行数据库软件的机器或者虚拟机称节点。每个节点独立使用本地的 CPU，内存和磁盘。节点之间的所有协调通信等任务全部运行在传统网络（以太网）之上且核心逻辑主要依靠软件来实现。

无共享系统不需要专门的硬件，具有较高的性价比。它可以跨多个地理区域分发数据，从而减少用户的访问延迟，甚至当整个数据中心发生灾难时仍能继续工作。通过云计算虚拟机的部署方式，即便是小公司，也可以轻松拥有跨区域的分布式架构和服务能力。



数据分布面临的主要问题

**复制与分区**

将数据分布在多节点时有两种常见的方式：

*复制*

在多个节点上保存相同数据的副本，每个副本具体的存储位置可能不尽相同。复制方法可以提供冗余：如果某些节点发生不可用，则可以通过其他节点继续提供数据访问服务。复制也可以帮助提高系统性能

*分区*

将一个大块头的数据库拆分成多个较小的子集即分区，不同的分区分配给不同的节点（也称为分片）。我们在第 6 章主要介绍分区技术。

## 五、数据复制

目的

* 使数据在地理位置上更接近用户，从而降低访问延迟。

* 当部分组件出现故障，系统依然可以继续工作，从而提高可用性。

* 扩展至多台机器以同时提供数据访问服务，从而提高读吞吐量。

假设单台机器可以保存数据集完整副本

问题：数据持续更改

三种流行的复制数据变化的方法：主从复制、多主节点复制和无主节点复制

### **主节点与从节点**

1. 指定某一个副本为主副本（或称为主节点）。当客户写数据库时，必须将写请求首先发送给主副本，主副本首先将新数据写入本地存储。

2. 其他副本则全部称为从副本（或称为从节点）。主副本把新数据写入本地存储后，然后将数据更改作为复制的日志或更改流发送给所有从副本。每个从副本获得更改日志之后将其应用到本地，且严格保持与主副本相同的写入顺序。

3. 客户端从数据库中读数据时，可以在主副本或者从副本上执行查询。再次强调，只有主副本才可以接受写请求；从客户端的角度来看，从副本都是只读的。

应用：数据库，消息队列

**同步复制与异步复制**

![](/images/image-18.png)

同步复制

优点：保证从节点读的时候数据最新

缺点：同步从节点无法确认影响写入成功

一般不会全部设置为同步复制，往往一个节点同步，其它从节点异步

**节点失效**

从节点失效：追赶恢复

从节点可以知道发生故障前处理的最后一笔事务，重新连接到主节点后，可以请求自那笔事务之后中断期间所有数据变更

主节点失效：节点切换

选择某个从节点提升为主节点，客户端之后的写请求发送到新的主节点，其它从节点接受新主节点上的变更数据

手动切换或自动切换

* 确认主节点失效

* 选举新的主节点

* 重新配置系统使新主节点生效，原先主节点恢复后降级

面临问题

* 异步复制新的主节点未收到原主节点数据就发生切换

* 两个节点同时自认为是主节点（脑裂）

* 设置合适的超时检测主节点失效



**复制日志实现**

1. 基于语句的复制

执行的每个写请求（操作语句）作为日志

不适用场景

* 调用非确定性函数语句，如 NOW（）

* 使用自增列或依赖于现有数据，副本需按照相同的顺序执行，当多个同时并发执行事务，有很大限制

* 有副作用的语句（例如，触发器、存储过程、用户定义的函数等），可能会在每个副本上产生不同的副作用

- 基于预写日志（WAL）传输

追加写日志

无论是日志结构存储引擎（SSTables 和 LSM-trees），还是修改预先写日志的 Btree，对数据库写入的字节序列都被介入日志

缺点：日志描述底层数据，包含了哪些磁盘块哪些字节改变，复制方案和存储引擎紧密耦合，如果数据库存储格式版本改变，系统通常无法支持主从节点软件版本不同

* 基于行的逻辑日志复制

复制和存储引擎采用不同的日志格式，通常是指一系列记录来描述数据表行级别的写请求

* 对于行插入，日志包含所有相关列的新值。

* 对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值。

* 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少包含所有已更新列的新值）。

Mysqlbinlog

逻辑日志和存储引擎逻辑解耦，可以容易地保持向后兼容

对外部应用程序（如用于离线分析的数据仓库），逻辑日志更容易解析

* 基于触发器的复制

注册自己的应用层代码，使得当数据库系统发生数据更改（写事务）时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑


### 复制滞后问题

主从复制延迟，具有最终一致性，但存在问题

**读自己的写**

用户写之后立马读，需要“写后读一致性”，能看到自己最近提交的更新，对其他用户没有保证

方案

* 访问可能被自己修改的内容，从主节点读取，否则在从节点读取，比如用户总是从主节点读自己的首页配置文件

* 跟踪最近更新的时间，如果更新后一分钟之内，在主节点读取，监控从节点复制滞后程度，避免从滞后时间超过 1 分钟从节点读取

* 客户端记住最近更新的时间戳，附带在读请求中，系统可以根据该信息确保提供的读至少包含了时间戳的更新

* 如果副本分布在多个数据中心，需路由到主节点所在数据中心

**单调读**

![](/images/image-22.png)

单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况

实现单调读的一种方式是，确保每个用户总是从固定的同一副本执行读取（而不同的用户可以从不同的副本读取）。例如，基于用户 ID 的哈希的方法而不是随机选择副本

**前缀一致读**

对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写人的顺序

![](/images/image-21.png)

如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常。然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值。

一个解决方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但该方案真实实现效率会大打折扣。现在有一些新的算法来显式地追踪事件因果关系，在本章稍后的“Happened-before 关系与并发”会继续该问题的探讨。

**解决方案**

应用层保证，例如只在主节点读取

事务保证

### 多主节点复制

配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似，处理写的每个主节点都必须将该数据更改转发到所有其他节点

适用场景

* 多数据中心，每个数据中心都配置主节点，性能更好（本地数据中心有主节点），容忍数据中心失效（数据中心故障可以不用切换主节点），容忍网络问题（多主一般用异步，容忍数据中心之间网络问题）

* 离线客户端操作，离线的更改会在下次设备联网时同步，每个设备都有一个充当主节点的本地数据库（接受写请求），联网后同步到服务端和其它设备

* 协作编辑，当一个用户编辑文档时，所做的更改会立即应用到本地副本（Web 浏览器或客户端应用程序），如果要确保不会发生编辑冲突，则应用程序必须先将文档锁定，然后才能对其进行编辑。锁来保证编辑不发生冲突。

**处理写冲突**

![](/images/image-29.png)

**冲突检测**

多主节点两个写请求只能在之后异步时间点检测到冲突，如果用同步冲突检测，失去了每个主节点独立接受写请求优势

**冲突避免**

确保相同记录写请求总是通过同一个主节点

**收敛于一致状态**

更改复制同步之后，确保所有副本的最终值是相同的

方式

1. 每个写入分配唯一的 ID

2. 每个副本分配唯一的 ID

3. 将值合并，比如 5-7 中的 B/C

4. 预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突（可能会提示用户）

**自定义冲突解决逻辑**

应用层解决冲突

写入时执行：复制变更日志检测到冲突，调用应用层的冲突处理程序

读取时执行：将冲突写入值都暂存，下次读取数据，会将数据多个版本返回给应用层。应用层可能会提示用户或自动解决冲突，并将最后的结果返回到数据库

**拓扑结构**

复制的拓扑结构描述了写请求从一个节点的传播到其他节点的通信路径

![](/images/image-24.png)

在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本。防止无限循环，每个节点赋予一个标识符，日志中每个写请求标记已通过节点标识符。如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，因此会忽略此变更请求，避免重复转发。

环形和星形拓扑的问题是，某个节点故障，可能会影响其他节点之间日志转发，可以重新配置拓扑结构排除故障节点。

全链接拓扑也存在一些自身的问题。主要是存在某些网络链路比其他链路更快的情况（例如由于不同网络拥塞），从而导致复制日志之间的覆盖，如图 5-9 所示。

![](/images/image-16.png)

更新依赖于之前的插入。日志仅添加时间戳还不够，无法确保时钟完全同步，无法在主节点 2 上正确排序日志

为了使得日志消息正确有序，可以使用一种称为版本向量的技术

### 无主节点复制

去中心复制，所有副本直接接受写请求

某些无主节点系统实现客户端写请求直接发送到多副本，其它一些由一个协调者节点代表客户端写入，与主节点数据库不同，协调者不负责写入顺序维护

**节点失效**

无需主节点切换，失效节点恢复后同步，通过版本号区分新旧值

![](/images/image-17.png)

**读修复与反熵**

读修复：并行读取多个副本，检测过期的返回值，发现过期写入新值

反熵：后台进程不断查找副本见数据差异，缺少的数据从一个副本复制到另一个副本，与主从复制日志不同，不保证以特定的顺序复制写人，并且会引入明显的同步滞后

**读写 quorum（限定数）**

r（读节点个数）>n（副本个数）-w（写入节点确认数）即可保证读取的节点中一定会包含最新值

常见选择：n 为奇数，w=r=（n+1）/2（向上舍入），也可灵活调整，如读多写少，设置 w=n 和 r=1 比较合适，这样读取速度更快，但是一个失效的节点就会使得数据库所有写人因无法完成 quorum 而失败

**Quorum 一致性的局限性**

通常设定 r 和 w 为简单多数，也可设置为较小的数字，w+r<=n，优点是等待节点数更少，并且允许更多副本无法访问情况依然能处理读取和写入

返回旧值情况

1. 写操作同时发生，需要处理写冲突

2. 写读同时发生，写仅在一部分副本完成

3. 总写入成功副本数小于 w，已成功副本不回滚，尽管写操作视作失败，后续读操作仍可能返回新值

……

**监控旧值**

监视是否返回最新结果，对于主从复制，维护复制日志执行的当前偏移量，对比主从节点当前偏移量的差值，衡量落后于主节点程度

对于无主节点复制系统，没有固定写入顺序，如果只支持读修复，旧值落后没有上限

**宽松的 quorum 与数据回传**

对于大规模集群（节点数>n）

宽松 quorum:只能连接到 n 个节点外的节点以满足 quorum，暂时写入到可访问的节点（不在 n 个节点中）

一旦网络问题得到解决，临时节点需要把接收到的写人全部发送到原始主节点上。这就是所谓的数据回传

提高写入可用性：只要有任何 w 个节点可用，数据库就可以接受新的写入。然而这意味着，即使满足 w+r>n，也不能保证在读取某个键时，一定能读到最新值，因为新值可能被临时写入 n 之外的某些节点且尚未回传过来

无法保证能从 r 个节点读到新值

**多数据中心**

无主节点复制也适用多数据中心

写入发送到所有副本，一般客户端等待本地数据中心 quorum 节点确认，远程数据中心通常配置为异步

**检测并发写**

![](/images/image-25.png)

如何处理写冲突

**最后写入者获胜（丢弃并发写入）（lastwritewins，LWW）**

无法确定写请求"自然顺序"，强制对其排序，例如为每个写请求附加一个时间戳，选择最新即最大的时间戳，丢弃较早时间戳的写入

会牺牲数据持久性，同一个主键有多个并发写，最后只有一个写入值存活

**Happens-before 关系和并发**

如何判断两个操作并发

如果 B 知道 A，或者依赖于 A，或者以某种方式在 A 基础上构建，则称操作 A 在操作 B 之前发生。这是定义何为并发的关键。事实上，我们也可以简单地说，如果两个操作都不在另一个之前发生，那么操作是并发的

> 并发性、时间和相对性
>
> 通常如果两个操作“同时”发生，则称之为并发，然而事实上，操作是否在时间上重叠并不重要。由于分布式系统中复杂的时钟同步问题（第 8 章将会详细讨论），现实当中，我们很难严格确定它们是否同时发生。
>
> 为更好地定义并发性，我们并不依赖确切的发生时间，即不管物理的时机如何，如果两个操作并不需要意识到对方，我们即可声称它们是并发操作。一些人尝试把这个思路与物理学中狭义相对论联系起来，后者引入了“信息传递不能超越光速”的假定，如果两个事件发生的间隔短于光在它们之间的折返，那么这两个事件不可能有相互影响，因此就是并发。
>
> 在计算机系统中，即使光速快到允许一个操作影响到另一个操作，但两个操作仍可能被定义为并发。例如，发生了网络拥塞或中断，可能就会出现两个操作由于网络问题导致一个操作无法感知另一个，因此二者成为并发

**确定前后关系**

![](/images/image-19.png)

* 服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号与写入的值一起保存。

* 当客户端读取主键时，服务器将返回所有（未被覆盖的）当前值以及最新的版本号。且要求写之前，客户必须先发送读请求。

* 客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合。写请求的响应可以像读操作一样，会返回所有当前值，这样就可以像购物车例子那样一步步链接起多个写入的值。

* 当服务器收到带有特定版本号的写入时，覆盖该版本号或更低版本的所有值（因为知道这些值已经被合并到新传入的值集合中），但必须保存更高版本号的所有值（因为这些值与当前的写操作属于并发）。

**合并并发值**

删除可能会出错，其中一个客户端删除，另一个没有，合并后出错

可以设计专门的数据结构自动执行合并，例如 Riak 的 CRDT 系列数据结构



## 六、数据分区

![](/images/diagram-1.png)

分区：每一条数据（或者每条记录，每行或每个文档）只属于某个特定分区

目的：提高可扩展性，大数据集分散在更多的磁盘上，查询负载分布，每个节点可以独立执行查询

**数据分区与数据复制**

分区通常与复制结合使用，即每个分区在多个节点都存有副本。这意味着某条记录属于特定的分区，而同样的内容会保存在不同的节点上以提高系统的容错性。

一个节点上可能存储了多个分区。图 6-1 展示了主从复制模型与分区组合使用时数据的分布情况。由图可知，每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。一个节点可能即是某些分区的主副本，同时又是其他分区的从副本。

![](/images/image-26.png)

### 键-值数据的分区

**基于关键字区间分区**

为每个分区分配一段连续的关键字或者关键字区间范围

每个分区内可以按照关键字排序保存，支持区间查询

**基于关键字哈希值分区**

避免数据倾斜，定义哈希函数

对于热点数据在关键字开头添加随机数，如 2 位随机数可以将写操作分布到 100 个关键字上，但读取也需要从所有 100 个关键字读取数据然后合并

### 分区与二级索引

二级索引不能唯一标识一条记录，只是用来加速特定值查询，挑战在于不能规整的映射到分区中

**基于文档分区的二级索引**

![](/images/image-20.png)

每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档而不关心其他分区中数据

每当需要写数据库时，包括添加、删除或更新文档等，只需要处理包含目标文档 ID 的那一个分区

读取需要发送到所有分区，尽量单个分区满足二级索引

**基于词条的二级索引分区**

对所有的数据构建全局索引，而不是每个分区维护自己的本地索引，但索引也需要分区

![](/images/image-27.png)

以待查找的关键字本身作为索引，可以通过关键词排序换分索引分区（如 a-r 在分区 0，s-z 在分区 1），也可以哈希划分

读取只用向包含词条的那一个分区发出读请求，写入会变慢，如果有多个二级索引并且分布在多个分区，导致写多个分区

一般不支持同步更新全局二级索引，采用异步更新

### 分区再平衡

* 查询压力增加，因此需要更多的 CPU 来处理负载。

* 数据规模增加，因此需要更多的磁盘和内存来存储数据。

* 节点可能出现故障，因此需要其他机器来接管失效的节点。

要求数据和请求可以从一个节点转移到另一个节点，这样一个迁移负载的过程称为再平衡

无论对于哪种分区方案，分区再平衡通常至少要满足：

* 平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。

* 再平衡执行过程中，数据库应该可以继续正常提供读写服务。

* 避免不必要的负载迁移，以加快动态再平衡，并尽量减少网络和磁盘 I/O 影响

**动态再平衡的策略**

为什么不取模？如果节点数变化，导致很多关键字节点迁移

**固定数量分区**

创建远超实际节点数的分区数，然后为每个节点分配多个分区

![](/images/image-15.png)

分区的数量往往在数据库创建时就确定好，之后不会改变。原则上也可以拆分和合并分区（稍后介绍），但固定数量的分区使得相关操作非常简单，因此许多采用固定分区策略的数据库决定不支持分区拆分功能。所以，在初始化时，已经充分考虑将来扩容增长的需求（未来可能拥有的最大节点数），设置一个足够大的分区数。而每个分区也有些额外的管理开销，分区数量过多可能会有副作用

如果数据集的总规模高度不确定或可变，此时如何选择合适的分区数就有些困难。每个分区包含的数据量的上限是固定的，实际大小应该与集群中的数据总量成正比。如果分区里的数据量非常大，则每次再平衡和节点故障恢复的代价就很大；但是如果一个分区太小，分区数量就会太多，产生太多的开销。分区大小应该“恰到好处”，不要太大，也不能过小，如果分区数量固定了但总数据量却高度不确定，就难以达到一个最佳取舍点。

**动态分区**

分区大小超过阈值，拆分为两个分区，分区缩小到某个阈值以下，与相邻分区合并

与固定数量的分区一样，每个分区总是分配给一个节点，而每个节点可以承载多个分区。

当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载

**按节点比例分区**

分区数与集群节点数成正比关系，每个节点具有固定数量的分区

当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系；当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法也使每个分区大小保持稳定

新增节点，随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点，要求采用基于哈希分区

### 请求路由

![](/images/image-35.png)

也可以依靠独立的协调服务（如 ZooKeeper）跟踪集群范围内的元数据

![](/images/image-30.png)

**并行查询执行**

对于大规模并行处理（massivelyparallelprocessing，MPP）这一类主要用于数据分析的关系数据库，在查询类型方面要复杂得多。典型的数据仓库查询包含多个联合、过滤、分组和聚合操作。MPP 查询优化器会将复杂的查询分解成许多执行阶段和分区，以便在集群的不同节点上并行执行。尤其是涉及全表扫描这样的查询操作，可以通过并行执行获益颇多。






## 九、一致性与共识

![](/images/diagram-4.png)

类比事务的抽象可以让应用程序可以假装没有崩溃（原⼦性），没有与其他⼈并发访问数据库（隔离性），且存储设备是完全可靠的（持久性），尝试建立可以让分布式系统应用忽略内部各种问题的抽象机制

**重要抽象之一：共识**

分布式系统可提供的若干保证和抽象机制  -->  解决共识问题的相关算法



### 一致性保证

最终一致性：停止更新数据库，等待一段时间后最终所有读请求会返回相同的内容。即不一致现象是暂时的，最终会达到一致

更强的一致性模型，意味着更多代价，如性能降低或容错性降低



### 可线性化

![非线性化例子](</images/截屏2025-06-27 14.35.10.png>)

可线性化基本思想：**使系统看起来好像只有一个数据副本**

![一个重要约束](</images/截屏2025-06-27 14.56.19.png>)

![](</images/截屏2025-06-27 15.01.41.png>)

CAP 理论，在出现网络故障时，一致性（线性化）和可用性无法兼具

线性化和性能之间的取舍



### 顺序保证

线性化的操作按某种顺序执行（图 9-4）

一些顺序要求的场景：

* 主从复制时，主节点确定复制日志写入顺序

* 可串行化确保的事务执行结果与按照某种顺序方式执行一样。

#### 顺序与因果关系

顺序的作用：保障因果关系

要求因果关系的一些场景：

* 图 5-5，先有问题的答案，再有问题的本身，违背了因果关系

* 图 5-9，需要先有创建数据行，后有更新

* 第 5 章“检测并发写”，两个操作 A 和 B，三种关系，A 发生在 B 之前，B 发生在 A 之前，A 和 B 并发，前两者都是因果关系

* 事务快照隔离，事务从一致性快照读取，一致性即与因果关系一直，如果快照包含了答案，那么它也应该包含所提的问题

* 事务写倾斜，图 7-8 调班动作的因果关系取决于当前值班人，可序列化的快照隔离（可串行化的快照隔离）主要通过跟踪事务之间的因果依赖关系来达到检测写倾斜目的

**因果顺序并⾮全序**

全序：支持任意两个元素的比较

因果：某两个事件的关系，一个发生在另一个前

#### 序列号排序

使用序列号排序事件

存在唯一主节点，可以让主节点为每个操作递增某个计数器，从而为复制日志中的每个操作赋值一个单增的序列号

**非因果序列发生器**

不存在唯一主节点，产生序列号较复杂

三个思路

1. 每个节点独立产生自己的一组序列号，比如 A 产生奇数，B 产生偶数，或者 A 产生 1-1000，B 产生 1000-2000

2. 时间戳信息（物理时钟）附加到操作上

问题：产生的序列号与因果关系不严格一致

**Lamport 时间戳**

值对（计数器，节点 ID）

![](/images/image-36.png)

最大计数器值嵌入到每一个请求中，如果发现收到的响应或请求中的最大值大于自身的技术器值，则把自己的计数器修改为该最大值

#### 全序关系广播

比喻：想象一个聊天群，里面有多个成员。全序关系广播要保证：无论消息从哪个成员发出，也无论每个成员在什么时间收到消息，所有成员看到的聊天记录顺序必须完全一致。

全序关系广播是一种消息传递协议，它确保在分布式系统的所有节点上，消息的传递顺序是完全一致的。它需要满足以下**两个核心属性**：

1. 可靠广播：如果一个正确的（非故障的）节点广播了一条消息，那么所有正确的节点最终都会收到这条消息。（不会丢失消息）

2. 全序传递：任何两个正确的节点，它们收到所有消息的顺序都是完全相同的。

**为什么需要它？**

在分布式系统（如数据库、状态机）中，多个副本需要保持完全一致的状态。实现这一目标最有效的方法就是让所有副本都以相同的顺序执行相同的操作指令。全序广播正是用来保证所有副本收到操作指令的顺序一致的机制。

实现技术：

常见的共识算法，如 Paxos 和 Raft，其核心功能之一就是实现全序关系广播。在 Raft 中，领导者（Leader）负责为所有客户端请求（即日志条目）确定一个唯一的顺序，并将这个顺序复制到所有追随者（Follower）节点上。



全序关系广播是实现线性化存储的一种强大工具。

* 如果你有一个全序关系广播系统，你可以很容易地构建一个线性化的存储。

  * 方法：将所有写操作（有时也包括读操作）通过全序广播发送给所有副本。每个副本按照广播确定的全局唯一顺序来依次执行这些操作。由于所有副本以相同顺序执行相同操作，它们的最终状态会保持一致，并且整个系统对外表现为线性化。



### 分布式事务与共识

需要达成一致的场景

* 主节点选举

* 原子事务提交

#### 原子提交和2PC（两阶段提交）

![](/images/image-32.png)

![](/images/image-31.png)

#### 实践中的分布式事务

##### **Exactly-once 消息处理**

* 在分布式系统中，消息可能因为网络抖动或服务故障而被**重复发送**。

* 如果处理逻辑不是幂等的（idempotent），重复处理会导致数据错误。

* Exactly-once 的目标是：**无论消息被发送多少次，处理结果都只发生一次且正确。**

**⚙️ 技术策略：如何实现 Exactly-once？**

1. **使用唯一消息 ID（Message ID）**

   * 每条消息附带一个唯一标识符。

   * 消费者记录已处理的 ID，避免重复处理。

2. **幂等操作（Idempotent Operations）**

   * 设计处理逻辑，使得重复执行不会改变最终结果。

   * 例如：数据库插入使用 UPSERT、更新使用 SET 而非 INCREMENT。

3. **事务性处理（Transactional Processing）**

   * 将消息消费与状态更新放入同一个事务中。

   * 如果事务失败，消息不会被标记为已处理。

4. **去重存储（Deduplication Store）**

   * 使用 Redis、数据库或日志系统记录已处理消息 ID。

   * 注意存储的 TTL 和性能影响。

**📌 实践挑战与权衡**

* **性能 vs. 精度**：Exactly-once 通常比 At-least-once 慢，因为需要额外的存储和检查。

* **状态一致性**：消费者必须维护处理状态，可能引入复杂的状态同步问题。

* **系统设计**：Kafka、RabbitMQ 等消息系统本身不保证 Exactly-once，需要应用层配合。

##### **XA交易**

**XA（eXtended Architecture）** 是由 X/Open 标准制定的分布式事务处理协议。

它用于在多个数据库或消息系统之间协调事务，确保满足 **ACID**（原子性、一致性、隔离性、持久性）属性。

支持 XA 的系统和工具

* 常见数据库：PostgreSQL、MySQL、DB2、SQL Server、Oracle 等。

* 消息中间件：ActiveMQ、HornetQ、MSMQ、IBM MQ 等

XA 使用 **两阶段提交协议（2PC）** 来确保所有参与者一致提交或回滚事务。

事务管理器负责协调各个资源的准备和提交过程。



##### **协调者故障中恢复**

当系统组件发生故障时，恢复不仅仅是重启，而是要**恢复到一致的状态**。

为了实现这一目标，系统需要依赖**日志记录**和**协调机制**。

**📚 关键概念拆解**

1. **日志的重要性**：

   * 每个组件记录自己的操作日志。

   * 恢复时通过重放日志来恢复状态。

   * 但日志可能不完整或顺序不一致，需协调。

2. **一致性判断的挑战**：

   * 系统需要判断哪些操作已经完成，哪些未完成。

   * 如果没有协调机制，可能会出现状态不一致或数据丢失。

3. **协调机制的作用**：

   * 协调多个组件的日志与状态，确保恢复后系统整体一致。

   * 类似于分布式事务中的两阶段提交（2PC）或更复杂的共识协议（如 Paxos、Raft）。

**实践中的恢复策略**

* **检查点（Checkpoint）**：定期保存系统状态，减少恢复时的复杂度。

* **幂等性设计**：确保操作可以重复执行而不会造成副作用。

* **恢复流程的验证**：系统设计中必须考虑如何验证恢复的正确性。

##### 分布式事务限制

* **分布式事务（如 XA）虽然能提供强一致性，但代价高昂**。

* 在大规模系统中，XA 的性能瓶颈和复杂性使其难以落地。

* 许多现代系统（如 HDFS、HBase）选择放弃 XA，采用更灵活的方式处理一致性。

**XA 的问题**：

* 实现复杂，依赖底层资源管理器的支持。

* 性能开销大，尤其在高并发场景下。

* 容错能力弱，一旦协调者故障，可能导致事务阻塞。

**2PC 的局限性**：

* **阻塞问题**：参与者在等待协调者指令时无法继续处理其他事务。

* **单点故障**：协调者崩溃后，系统可能陷入不确定状态。

* **缺乏自动恢复机制**：无法应对网络分区或节点重启后的状态恢复。

**一致性 vs 可用性**：

* 在 CAP 理论下，强一致性往往牺牲了可用性。

* 某些系统选择最终一致性或幂等操作来提升可用性。



#### 支持容错的共识

* 在分布式系统中，多个节点需要就某个事实达成一致（consensus），即使存在网络延迟、节点故障或恶意行为。

* 共识协议的目标是确保系统在不可靠环境下仍能做出一致决策。

**📐 共识协议的基本性质**

1. **一致性（Uniform Agreement）**

   * 所有非故障节点最终必须达成相同的决策。

2. **有效性（Validity）**

   * 如果某个节点提出了一个值，最终决定的值必须是某个节点曾提出过的。

3. **终止性（Termination）**

   * 所有非故障节点最终都必须做出决定，不能无限等待。

##### 共识算法与全序广播

* **共识算法（Consensus Algorithm）**：用于在多个节点之间就某个值达成一致，确保系统状态一致性。

* **全序广播（Total Order Broadcast）**：确保所有节点以相同顺序接收并处理消息，是实现状态机复制的关键。

两者密切相关：实现全序广播的本质就是实现共识。

**⚙️ 核心协议对比：VSR、Paxos、Raft、Zab**

**一致性与共识的要点**

1. **节点可能崩溃，消息可能丢失** → 共识算法必须具备容错能力。

2. **容错的基础是消息不能丢失** → 需持久化日志、重试机制。

3. **VSR、Raft、Zab 的决策需稳定议员确认** → 强调领导者与多数派的角色。

4. **Paxos 的决策更灵活** → 只要满足 2M 的条件（多数派），任何节点可提议。

##### 主从复制与共识

在主从架构中，**所有写操作由主节点处理**，然后同步到从节点。

表面上看，主节点决定一切，似乎不需要共识。

但实际上，为了保证系统的**一致性与可靠性**，共识机制仍然不可或缺。

1. **主节点的决策是否可靠？**

* 如果主节点出现故障或行为异常（如脑裂），可能导致数据不一致。

* 因此，系统需要一种机制来验证主节点的决策是否被多数节点认可。

- **主节点的选举过程**

* 当主节点失效时，系统必须选出新的主节点。

* 这个过程本质上就是一个**共识问题**：多个节点必须就“谁是新主”达成一致。

* 常见协议如 Raft、Zab 都将主节点选举作为核心部分。

- **复制策略与一致性保障**

* 主节点在写入数据后，需将变更同步到从节点。

* 若同步机制不具备确认机制（如 ACK），可能导致数据丢失或不一致。

* 共识协议可以确保写入在多数节点上达成一致后才算成功。

##### Epoch和Quorum

1. Epoch（世代编号）

* **Epoch Number / Term Number / Ballot Number**：

  * 都是用于标识某一轮共识或领导者任期的编号。

  * 在 Raft 中称为 Term，在 Paxos 中称为 Ballot。

* **作用**

  * 将系统时间切分为不重叠的世代（epoch）

  * 每个 epoch 内 Leader 唯一且确定

  * 单调递增，保证新的 Leader 一定“更新”于旧 Leader

- Leader 选举与确认流程

  1. **故障检测**

     * 若怀疑当前 Leader 失效，发起新一轮选举

  2. **新 Leader 产生**

     * 投票产生一个更高的 epoch 编号

     * 如果出现两个 Leader → 编号高者胜

  3. **Leader 决策前检查**

     * 必须确认没有比自己更新的 epoch Leader 存在

     * 自身“认为是 Leader”并不够，需要 Quorum 认可

- Quorum（法定人数）要求

* **定义**：一个足够大的节点集合，能在决策上代表整个集群

* **常见情况**：多数节点（N/2 + 1），但并非必须是简单多数

* **作用**：

  * Leader 确认自己合法性

  * 对提案进行投票表决

* **关键约束**：选举 Quorum 与提案投票 Quorum 必须有 **重叠节点**，确保安全性（防止两个不同 epoch 的决议同时被提交）

- 两轮投票

  1. **轮 1**：选举 Leader（产生 epoch）

  2. **轮 2**：Leader 提案投票（带 epoch 号）

  * 如果提案投票中未出现更高的 epoch，说明 Leader 没被取代 → 提案安全可提交

##### 共识的局限性

**共识的优势与代价**

**✅ 优势（前提背景）**

* **安全属性**：一致性（Consistency）、完整性（Integrity）、有效性（Validity）

* **容错能力**：多数节点存活即可继续服务

* **线性化保证**：通过全序关系广播（Total Order Broadcast）实现原子操作的线性化语义

**&#x20;限制与成本**

#### 成员与协调服务

**类型**：分布式键值存储（Distributed KV Store）/ 协调与配置服务（Coordination & Configuration Service）

**主要用途**：为分布式系统提供可靠的小规模元数据存储与一致性保证

**非目标**：存储大量业务数据（其数据量可全部放入内存，磁盘仅用于持久化）

1. **为什么需要共识**

2. **🔑 核心特性**

   1. **线性化原子操作 (Linearizable Atomic Ops)**

      * 例如：CAS（Compare-And-Set）实现分布式锁

      * 结合租约（lease）机制，防止客户端失效后锁长期占用

   2. **操作全序 (Total Order)**

      * 所有操作赋予单调递增的事务 ID (zxid) + 版本号

      * 支持 fencing token 防止进程暂停导致的锁冲突

   3. **故障检测 (Failure Detection)**

      * 会话与心跳机制（Session + Heartbeat）

      * 会话失效 → 自动删除临时节点 (ephemeral nodes) → 释放锁资源

   4. **变更通知 (Watch/Notification)**

      * 客户端可订阅节点变化（成员加入、离开、数据变化）

      * 避免频繁轮询

3. &#x20;依赖场景（生态中的作用）

很多分布式系统（HBase, Hadoop YARN, Kafka, OpenStack Nova 等）依赖 ZK/etcd 去解决：

* 集群成员管理（Membership Management）

* Leader 选举

* 分布式锁/租约

* 配置分发

* 元数据一致性

##### 节点任务分配

1. 核心场景

**单主故障切换（Leader Failover）**

* 多个实例中始终保持一个主节点

* 主挂掉时由其他节点接管

* 常见于主从复制数据库、作业调度器、有状态服务

**分区资源分配（Partition Assignment）**

* 多个分区（DB 分片、消息流、文件块、Actor 等）分配给不同节点

* 新节点加入 → 迁移部分分区实现动态负载均衡

* 节点离开/失效 → 其他节点接管其分区

- ZooKeeper 提供的能力支撑

- 架构上的取舍

* **外包协调**：应用不需要自己实现共识、全序广播、故障检测 → 降低复杂度

* **数据变化频率低**：例如 “分区7的主节点在 10.1.1.23” 这种信息，分钟/小时级更新即可

* **不适合高频实时状态**：如果是每秒数千到百万级变化（例如指标流、事件流），应考虑 BookKeeper、Kafka 这种流式或日志存储

##### 服务发现

1. 核心场景：服务发现的角色

* **目标**：让客户端动态找到可用服务的地址（IP/端口等）

* **挑战**：云环境中节点常常会频繁上下线，IP 动态变化，启动前很难提前知道彼此的位置

* **典型实现**：

  1. 节点启动时向 ZooKeeper / etcd / Consul 注册自身网络信息

  2. 其他组件通过查询注册表获取当前可用的服务地址

- 共识在服务发现中的取舍

> 现实中，DNS 的设计哲学是**可用性优先**：即使网络中断时依然尽可能返回缓存结果。这种策略适合多数服务发现场景。

* 🛠 共识系统与只读副本

- 某些共识系统支持 **只读缓存副本**（Read-only Replica）

  * 异步接收由投票节点达成的决议日志

  * 自身不参与投票

  * 适合提供不需要强一致的读服务（如非关键的服务发现查询）

##### 成员服务

1. **🧩 成员服务的核心概念**

   * 用于确定“当前集群有效成员”的系统组件

   * 常回答两个问题：

     1. 哪些节点被视为集群的一部分

     2. 这些节点目前是否处于“活跃”状态

2. **关键特点**

   1. **与故障检测绑定的共识**

      * 单纯的网络心跳 ≠ 可靠故障判断（因网络延迟不可避免）

      * 通过共识让所有存活节点**一致认定**某个节点是否失效

      * 即便存在误判，保证全体决策一致性仍是首要目标 （避免不同节点眼中的“集群成员列表”不一致）

   2. **典型应用场景**

      * **Leader 选举**：如选择编号最小的节点为主

      * **集群 reconfiguration**：添加、移除节点需要全体一致

      * **任务分配 / 决策投票**：前提是所有参与方对“谁在场”有相同认知

   3. **共识失败条件**

      * 若各节点对“当前集群成员集”存在分歧 → 共识过程无法继续

      * 因为协议参与方集合不一致会破坏安全性保证（投票多数派定义失效）

3. **📌 和之前讨论内容的联系**

   * **与协调服务的关系**

     * ZooKeeper / etcd 提供了内置的成员服务（Membership）能力

     * 这层能力是 Leader 选举、分区分配、锁服务的前提条件

   * **与 Epoch / Quorum 的关系**

     * Epoch 机制默认节点集合固定，一旦动态调整就需要成员服务介入

     * Quorum 的定义依赖于“有效成员集”的共识结果
