---
title: 数据密集型应用系统设计-第五章-数据复制
date: 2025-09-28 20:39:32
tags:
   - "系统设计"
categories:
   - [Study-Note, 数据密集型应用系统设计]
description: 数据密集型应用系统设计-第五章-数据复制
keywords: 数据密集型应用系统设计,数据复制
---

# 分布式数据系统

分布式目的

* 扩展性，单机器处理能力有限

* 容错与高可用性

* 延迟考虑，靠近客户端



**系统扩展**

垂直扩展和水平扩展

* 共享内存架构

由一个操作系统管理更多的 CPU，内存和磁盘，通过高速内部总线使每个 CPU 都可以访问所有的存储器或磁盘，容错有限，无法提供异地容错能力

成本增长过快甚至超过了线性：即如果把一台机器内的 CPU 数量增加一倍，内存扩容一倍，磁盘容量加大一倍，则最终总成本增加不止一倍。并且由于性能瓶颈因素，这样一台机器尽管拥有了两倍的硬件指标但却不一定能处理两倍的负载。

* 共享磁盘架构

拥有多台服务器，每个服务器各自拥有独立的 CPU 和内存，然后将数据存储在可共享访问的磁盘阵列上，服务器与磁盘阵列之间往往通过高速网络连接。

这种架构多适用于数据仓库等负载，然而通常由于资源竞争以及锁的开销等限制了其进一步的扩展能力

* 无共享架构（水平扩展）

运行数据库软件的机器或者虚拟机称节点。每个节点独立使用本地的 CPU，内存和磁盘。节点之间的所有协调通信等任务全部运行在传统网络（以太网）之上且核心逻辑主要依靠软件来实现。

无共享系统不需要专门的硬件，具有较高的性价比。它可以跨多个地理区域分发数据，从而减少用户的访问延迟，甚至当整个数据中心发生灾难时仍能继续工作。通过云计算虚拟机的部署方式，即便是小公司，也可以轻松拥有跨区域的分布式架构和服务能力。



数据分布面临的主要问题

**复制与分区**

将数据分布在多节点时有两种常见的方式：

*复制*

在多个节点上保存相同数据的副本，每个副本具体的存储位置可能不尽相同。复制方法可以提供冗余：如果某些节点发生不可用，则可以通过其他节点继续提供数据访问服务。复制也可以帮助提高系统性能

*分区*

将一个大块头的数据库拆分成多个较小的子集即分区，不同的分区分配给不同的节点（也称为分片）。我们在第 6 章主要介绍分区技术。

## 五、数据复制

![数据复制](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-40.png)
目的

* 使数据在地理位置上更接近用户，从而降低访问延迟。

* 当部分组件出现故障，系统依然可以继续工作，从而提高可用性。

* 扩展至多台机器以同时提供数据访问服务，从而提高读吞吐量。

假设单台机器可以保存数据集完整副本

问题：数据持续更改

三种流行的复制数据变化的方法：主从复制、多主节点复制和无主节点复制

### **主节点与从节点**

1. 指定某一个副本为主副本（或称为主节点）。当客户写数据库时，必须将写请求首先发送给主副本，主副本首先将新数据写入本地存储。

2. 其他副本则全部称为从副本（或称为从节点）。主副本把新数据写入本地存储后，然后将数据更改作为复制的日志或更改流发送给所有从副本。每个从副本获得更改日志之后将其应用到本地，且严格保持与主副本相同的写入顺序。

3. 客户端从数据库中读数据时，可以在主副本或者从副本上执行查询。再次强调，只有主副本才可以接受写请求；从客户端的角度来看，从副本都是只读的。

应用：数据库，消息队列

**同步复制与异步复制**

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-18.png)

同步复制

优点：保证从节点读的时候数据最新

缺点：同步从节点无法确认影响写入成功

一般不会全部设置为同步复制，往往一个节点同步，其它从节点异步

**节点失效**

从节点失效：追赶恢复

从节点可以知道发生故障前处理的最后一笔事务，重新连接到主节点后，可以请求自那笔事务之后中断期间所有数据变更

主节点失效：节点切换

选择某个从节点提升为主节点，客户端之后的写请求发送到新的主节点，其它从节点接受新主节点上的变更数据

手动切换或自动切换

* 确认主节点失效

* 选举新的主节点

* 重新配置系统使新主节点生效，原先主节点恢复后降级

面临问题

* 异步复制新的主节点未收到原主节点数据就发生切换

* 两个节点同时自认为是主节点（脑裂）

* 设置合适的超时检测主节点失效



**复制日志实现**

1. 基于语句的复制

执行的每个写请求（操作语句）作为日志

不适用场景

* 调用非确定性函数语句，如 NOW（）

* 使用自增列或依赖于现有数据，副本需按照相同的顺序执行，当多个同时并发执行事务，有很大限制

* 有副作用的语句（例如，触发器、存储过程、用户定义的函数等），可能会在每个副本上产生不同的副作用

- 基于预写日志（WAL）传输

追加写日志

无论是日志结构存储引擎（SSTables 和 LSM-trees），还是修改预先写日志的 Btree，对数据库写入的字节序列都被介入日志

缺点：日志描述底层数据，包含了哪些磁盘块哪些字节改变，复制方案和存储引擎紧密耦合，如果数据库存储格式版本改变，系统通常无法支持主从节点软件版本不同

* 基于行的逻辑日志复制

复制和存储引擎采用不同的日志格式，通常是指一系列记录来描述数据表行级别的写请求

* 对于行插入，日志包含所有相关列的新值。

* 对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值。

* 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少包含所有已更新列的新值）。

Mysqlbinlog

逻辑日志和存储引擎逻辑解耦，可以容易地保持向后兼容

对外部应用程序（如用于离线分析的数据仓库），逻辑日志更容易解析

* 基于触发器的复制

注册自己的应用层代码，使得当数据库系统发生数据更改（写事务）时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑


### 复制滞后问题

主从复制延迟，具有最终一致性，但存在问题

**读自己的写**

用户写之后立马读，需要“写后读一致性”，能看到自己最近提交的更新，对其他用户没有保证

方案

* 访问可能被自己修改的内容，从主节点读取，否则在从节点读取，比如用户总是从主节点读自己的首页配置文件

* 跟踪最近更新的时间，如果更新后一分钟之内，在主节点读取，监控从节点复制滞后程度，避免从滞后时间超过 1 分钟从节点读取

* 客户端记住最近更新的时间戳，附带在读请求中，系统可以根据该信息确保提供的读至少包含了时间戳的更新

* 如果副本分布在多个数据中心，需路由到主节点所在数据中心

**单调读**

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-22.png)

单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况

实现单调读的一种方式是，确保每个用户总是从固定的同一副本执行读取（而不同的用户可以从不同的副本读取）。例如，基于用户 ID 的哈希的方法而不是随机选择副本

**前缀一致读**

对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写人的顺序

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-21.png)

如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常。然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值。

一个解决方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但该方案真实实现效率会大打折扣。现在有一些新的算法来显式地追踪事件因果关系，在本章稍后的“Happened-before 关系与并发”会继续该问题的探讨。

**解决方案**

应用层保证，例如只在主节点读取

事务保证

### 多主节点复制

配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似，处理写的每个主节点都必须将该数据更改转发到所有其他节点

适用场景

* 多数据中心，每个数据中心都配置主节点，性能更好（本地数据中心有主节点），容忍数据中心失效（数据中心故障可以不用切换主节点），容忍网络问题（多主一般用异步，容忍数据中心之间网络问题）

* 离线客户端操作，离线的更改会在下次设备联网时同步，每个设备都有一个充当主节点的本地数据库（接受写请求），联网后同步到服务端和其它设备

* 协作编辑，当一个用户编辑文档时，所做的更改会立即应用到本地副本（Web 浏览器或客户端应用程序），如果要确保不会发生编辑冲突，则应用程序必须先将文档锁定，然后才能对其进行编辑。锁来保证编辑不发生冲突。

**处理写冲突**

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-29.png)

**冲突检测**

多主节点两个写请求只能在之后异步时间点检测到冲突，如果用同步冲突检测，失去了每个主节点独立接受写请求优势

**冲突避免**

确保相同记录写请求总是通过同一个主节点

**收敛于一致状态**

更改复制同步之后，确保所有副本的最终值是相同的

方式

1. 每个写入分配唯一的 ID

2. 每个副本分配唯一的 ID

3. 将值合并，比如 5-7 中的 B/C

4. 预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突（可能会提示用户）

**自定义冲突解决逻辑**

应用层解决冲突

写入时执行：复制变更日志检测到冲突，调用应用层的冲突处理程序

读取时执行：将冲突写入值都暂存，下次读取数据，会将数据多个版本返回给应用层。应用层可能会提示用户或自动解决冲突，并将最后的结果返回到数据库

**拓扑结构**

复制的拓扑结构描述了写请求从一个节点的传播到其他节点的通信路径

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-24.png)

在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本。防止无限循环，每个节点赋予一个标识符，日志中每个写请求标记已通过节点标识符。如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，因此会忽略此变更请求，避免重复转发。

环形和星形拓扑的问题是，某个节点故障，可能会影响其他节点之间日志转发，可以重新配置拓扑结构排除故障节点。

全链接拓扑也存在一些自身的问题。主要是存在某些网络链路比其他链路更快的情况（例如由于不同网络拥塞），从而导致复制日志之间的覆盖，如图 5-9 所示。

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-16.png)

更新依赖于之前的插入。日志仅添加时间戳还不够，无法确保时钟完全同步，无法在主节点 2 上正确排序日志

为了使得日志消息正确有序，可以使用一种称为版本向量的技术

### 无主节点复制

去中心复制，所有副本直接接受写请求

某些无主节点系统实现客户端写请求直接发送到多副本，其它一些由一个协调者节点代表客户端写入，与主节点数据库不同，协调者不负责写入顺序维护

**节点失效**

无需主节点切换，失效节点恢复后同步，通过版本号区分新旧值

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-17.png)

**读修复与反熵**

读修复：并行读取多个副本，检测过期的返回值，发现过期写入新值

反熵：后台进程不断查找副本见数据差异，缺少的数据从一个副本复制到另一个副本，与主从复制日志不同，不保证以特定的顺序复制写人，并且会引入明显的同步滞后

**读写 quorum（限定数）**

r（读节点个数）>n（副本个数）-w（写入节点确认数）即可保证读取的节点中一定会包含最新值

常见选择：n 为奇数，w=r=（n+1）/2（向上舍入），也可灵活调整，如读多写少，设置 w=n 和 r=1 比较合适，这样读取速度更快，但是一个失效的节点就会使得数据库所有写人因无法完成 quorum 而失败

**Quorum 一致性的局限性**

通常设定 r 和 w 为简单多数，也可设置为较小的数字，w+r<=n，优点是等待节点数更少，并且允许更多副本无法访问情况依然能处理读取和写入

返回旧值情况

1. 写操作同时发生，需要处理写冲突

2. 写读同时发生，写仅在一部分副本完成

3. 总写入成功副本数小于 w，已成功副本不回滚，尽管写操作视作失败，后续读操作仍可能返回新值

……

**监控旧值**

监视是否返回最新结果，对于主从复制，维护复制日志执行的当前偏移量，对比主从节点当前偏移量的差值，衡量落后于主节点程度

对于无主节点复制系统，没有固定写入顺序，如果只支持读修复，旧值落后没有上限

**宽松的 quorum 与数据回传**

对于大规模集群（节点数>n）

宽松 quorum:只能连接到 n 个节点外的节点以满足 quorum，暂时写入到可访问的节点（不在 n 个节点中）

一旦网络问题得到解决，临时节点需要把接收到的写人全部发送到原始主节点上。这就是所谓的数据回传

提高写入可用性：只要有任何 w 个节点可用，数据库就可以接受新的写入。然而这意味着，即使满足 w+r>n，也不能保证在读取某个键时，一定能读到最新值，因为新值可能被临时写入 n 之外的某些节点且尚未回传过来

无法保证能从 r 个节点读到新值

**多数据中心**

无主节点复制也适用多数据中心

写入发送到所有副本，一般客户端等待本地数据中心 quorum 节点确认，远程数据中心通常配置为异步

**检测并发写**

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-25.png)

如何处理写冲突

**最后写入者获胜（丢弃并发写入）（lastwritewins，LWW）**

无法确定写请求"自然顺序"，强制对其排序，例如为每个写请求附加一个时间戳，选择最新即最大的时间戳，丢弃较早时间戳的写入

会牺牲数据持久性，同一个主键有多个并发写，最后只有一个写入值存活

**Happens-before 关系和并发**

如何判断两个操作并发

如果 B 知道 A，或者依赖于 A，或者以某种方式在 A 基础上构建，则称操作 A 在操作 B 之前发生。这是定义何为并发的关键。事实上，我们也可以简单地说，如果两个操作都不在另一个之前发生，那么操作是并发的

> 并发性、时间和相对性
>
> 通常如果两个操作“同时”发生，则称之为并发，然而事实上，操作是否在时间上重叠并不重要。由于分布式系统中复杂的时钟同步问题（第 8 章将会详细讨论），现实当中，我们很难严格确定它们是否同时发生。
>
> 为更好地定义并发性，我们并不依赖确切的发生时间，即不管物理的时机如何，如果两个操作并不需要意识到对方，我们即可声称它们是并发操作。一些人尝试把这个思路与物理学中狭义相对论联系起来，后者引入了“信息传递不能超越光速”的假定，如果两个事件发生的间隔短于光在它们之间的折返，那么这两个事件不可能有相互影响，因此就是并发。
>
> 在计算机系统中，即使光速快到允许一个操作影响到另一个操作，但两个操作仍可能被定义为并发。例如，发生了网络拥塞或中断，可能就会出现两个操作由于网络问题导致一个操作无法感知另一个，因此二者成为并发

**确定前后关系**

![](https://cdn.jsdelivr.net/gh/1935Zz/1935zz.github.io@main/source/img/data-intensive/image-19.png)

* 服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，并将新版本号与写入的值一起保存。

* 当客户端读取主键时，服务器将返回所有（未被覆盖的）当前值以及最新的版本号。且要求写之前，客户必须先发送读请求。

* 客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合。写请求的响应可以像读操作一样，会返回所有当前值，这样就可以像购物车例子那样一步步链接起多个写入的值。

* 当服务器收到带有特定版本号的写入时，覆盖该版本号或更低版本的所有值（因为知道这些值已经被合并到新传入的值集合中），但必须保存更高版本号的所有值（因为这些值与当前的写操作属于并发）。

**合并并发值**

删除可能会出错，其中一个客户端删除，另一个没有，合并后出错

可以设计专门的数据结构自动执行合并，例如 Riak 的 CRDT 系列数据结构



